{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/cross_validation.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# I didn't wind up using this in my submissions, but I think this is a better CV splitter, I just ran out of time because of XGB issues.\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import datetime\n",
    "from itertools import product\n",
    "from scipy import interpolate ## For other interpolation functions.\n",
    "import time\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "from sklearn.cross_validation import LabelKFold\n",
    "\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with gzip.open('merged7.pkl.gz', 'rb') as fd:\n",
    "    data = pickle.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testset = np.where(data['outcome'].isnull())\n",
    "test = data.iloc[testset]\n",
    "\n",
    "trainset = np.where(~data['outcome'].isnull())\n",
    "train = data.iloc[trainset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29281 1393 3550 756155\n"
     ]
    }
   ],
   "source": [
    "# compute the 'second' leak\n",
    "\n",
    "# ALL test targets with 0.0/1.0 average are fully/accurately inferred in the test set\n",
    "\n",
    "grouplist_all0 = []\n",
    "grouplist_all1 = []\n",
    "grouplist = []\n",
    "count = 0\n",
    "\n",
    "for g in data.groupby(['group_1'], sort=False):\n",
    "    if len(g[1]) > 50:\n",
    "        m = g[1].outcome.mean()\n",
    "        if m != 0 and m != 1:\n",
    "            #print(g[0], len(g[1]), g[1].outcome.mean())\n",
    "            grouplist.append(g[0])\n",
    "            \n",
    "            count += len(g[1])\n",
    "        elif m == 0:\n",
    "            grouplist_all0.append(g[0])\n",
    "        elif m == 1:\n",
    "            grouplist_all1.append(g[0])\n",
    "    else:\n",
    "        grouplist.append(g[0])\n",
    "            \n",
    "        count += len(g[1])\n",
    "            \n",
    "print(len(grouplist), len(grouplist_all0), len(grouplist_all1), count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask0 = np.full(len(data), False, dtype=np.bool)\n",
    "mask1 = np.full(len(data), False, dtype=np.bool)\n",
    "for g in grouplist_all0:\n",
    "    mask0 = np.logical_or(mask0, data.group_1 == g)\n",
    "    #train.outcome_filled.values[np.where(mask)] = .05\n",
    "for g in grouplist_all1:\n",
    "    mask1 = np.logical_or(mask1, data.group_1 == g)\n",
    "    #train.outcome_filled.values[np.where(mask)] = .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['outcome_leak2'] = np.full_like(data['outcome'].values, np.nan)\n",
    "\n",
    "data.loc[mask0, 'outcome_leak2'] = .05\n",
    "data.loc[mask1, 'outcome_leak2'] = .95\n",
    "\n",
    "testset = np.where(data['outcome'].isnull())\n",
    "test = data.iloc[testset]\n",
    "\n",
    "trainset = np.where(~data['outcome'].isnull())\n",
    "train = data.iloc[trainset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leak2_mask = np.logical_or(mask0, mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = train.columns.copy()\n",
    "cols = cols.drop('activity_id')\n",
    "train_dups = train.duplicated(subset=cols)\n",
    "train_dedup = train[~train_dups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1213878 2197291\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dedup), len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "stratkey = 'people_id'\n",
    "\n",
    "balls = []\n",
    "for p in train_dedup.groupby([stratkey]):\n",
    "    balls.append([p[0], len(p[1]), p[1].outcome.mean()])\n",
    "\n",
    "balls = sorted(balls, key=itemgetter(2), reverse=True)\n",
    "\n",
    "# This assumes vc is sorted by whatever you want stratified\n",
    "def dosplit_rr(df, vc, folds, fuzz = (43254, .5, 4)):\n",
    "    if fuzz is not None:\n",
    "        np.random.seed(fuzz[0])\n",
    "    \n",
    "    bcount = np.zeros(folds)\n",
    "    \n",
    "    buckets = []\n",
    "    for f in range(folds):\n",
    "        buckets.append([])\n",
    "    \n",
    "    ballpit = copy.deepcopy(balls)\n",
    "    \n",
    "    runs = 0\n",
    "    \n",
    "    tot = 0\n",
    "    \n",
    "    while len(ballpit):\n",
    "        runs += 1\n",
    "        sel = 0\n",
    "        r = np.random.rand()\n",
    "        if r < fuzz[1]:\n",
    "            sel = int((fuzz[2] / fuzz[1]) * r)\n",
    "            if sel >= len(ballpit):\n",
    "                sel = len(ballpit) - 1\n",
    "\n",
    "        tot += sel\n",
    "                \n",
    "        v = ballpit[sel]\n",
    "        del ballpit[sel]\n",
    "                \n",
    "        selbucket = np.argsort(bcount)[0]\n",
    "        \n",
    "        buckets[selbucket].append(v[0])\n",
    "        bcount[selbucket] += v[1]\n",
    "    \n",
    "    print(len(balls), runs, tot)\n",
    "    \n",
    "    return buckets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151295 151295 113607\n"
     ]
    }
   ],
   "source": [
    "folds = 8\n",
    "pidsets_grouped = dosplit_rr(train_dedup, balls, folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pidset = pidsets_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_train = []\n",
    "cv_val = []\n",
    "cv_val_tgt = []\n",
    "\n",
    "cv_val_dups, cv_val_dups_tgt = [], []\n",
    "\n",
    "pu = []\n",
    "\n",
    "for p in pidset:\n",
    "    cv_train.append(train_dedup[~train_dedup.people_id.isin(p)])\n",
    "\n",
    "    cv_val.append(train_dedup[train_dedup.people_id.isin(p)])\n",
    "    cv_val_tgt.append(cv_val[-1][['activity_id', 'outcome']].copy())\n",
    "    \n",
    "    pu.append(list(cv_val[-1].people_id.unique()))\n",
    "\n",
    "    cv_val_dups.append(train[train.people_id.isin(p)])\n",
    "    cv_val_dups_tgt.append(cv_val_dups[-1][['activity_id', 'outcome']].copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 151730 0.48606076583404734 277430 0.43994881591752877\n",
      "1 151737 0.4860383426586792 256566 0.50167208437595\n",
      "2 151740 0.48605509424014764 293694 0.4056535033061622\n",
      "3 151727 0.4860703763997179 306867 0.3978205541814532\n",
      "4 151733 0.48605115564840873 252029 0.476675303238913\n",
      "5 151739 0.4860319364171373 252860 0.477449972316697\n",
      "6 151742 0.48604209777121693 254765 0.4735933114831315\n",
      "7 151730 0.4860805377974033 303080 0.40251748713211033\n"
     ]
    }
   ],
   "source": [
    "for k in range(len(cv_val)):\n",
    "    print(k, len(cv_val_tgt[k]), cv_val_tgt[k].outcome.mean(), len(cv_val_dups_tgt[k]), cv_val_dups_tgt[k].outcome.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process dates\n",
    "\n",
    "if True: # Wasteful to recompute a constant every time\n",
    "    minactivdate = pd.Timestamp('2022-07-17 00:00:00')\n",
    "    maxactivdate = pd.Timestamp('2023-08-31 00:00:00')\n",
    "else:\n",
    "    minactivdate = min(activs['date'])\n",
    "    maxactivdate = max(activs['date'])\n",
    "\n",
    "alldays = [maxactivdate - datetime.timedelta(days=x) for x in range(0, (maxactivdate - minactivdate).days+1)][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fv = []\n",
    "\n",
    "def interpolateFun0(x):\n",
    "    \"\"\"Original script author's function rewritten in Python.\n",
    "    The author interpolates between two known values by averaging them. We\n",
    "    can think of this as 0th order interpolation. \"\"\"\n",
    "\n",
    "    ## TODO: This function could use some optimization. The R version is much faster...\n",
    "    x = x.reset_index(drop=True)\n",
    "    g = x['outcome'].copy() ## g should be a list or a pandas Series.\n",
    "    \n",
    "    global fv\n",
    "\n",
    "    if (g.shape[0] < 3): ## If we have at most two rows.\n",
    "        x['outcome_filled'] = g ## Will be replaced by a mean.\n",
    "#        x['outcome'] = x['filled']\n",
    "        return x\n",
    "    \n",
    "    if np.sum(g.isnull()) == 0:\n",
    "        x['outcome_filled'] = g\n",
    "        return x\n",
    "    \n",
    "    out = g.values.copy()\n",
    "    value_locs = np.where(~g.isnull())[0]\n",
    "    \n",
    "    if len(value_locs) == 0:\n",
    "        x['outcome_filled'] = np.full_like(out, np.nan)\n",
    "#        x['outcome'] = x['filled']\n",
    "        return x\n",
    "    \n",
    "    if len(value_locs) == 1:\n",
    "        fillval = .89 if (g[value_locs[0]] == 1) else .13\n",
    "        fv.append((g[value_locs[0]], fillval))\n",
    "        g[g.isnull()] = fillval\n",
    "\n",
    "        x['outcome_filled'] = g\n",
    "#        x['outcome'] = x['filled']\n",
    "\n",
    "        return x        \n",
    "    \n",
    "    # Fill in beginning (if needed)\n",
    "    if value_locs[0]:\n",
    "        \n",
    "        fillval = .89 if (g[value_locs[0]] == 1) else .13\n",
    "        fv.append((g[value_locs[0]], fillval))\n",
    "        \n",
    "        out[0:value_locs[0]] = fillval\n",
    "\n",
    "    # Interpolate holes in the middle\n",
    "    for i in range(0, len(value_locs) - 1):\n",
    "        beg = value_locs[i]\n",
    "        end = value_locs[i + 1]\n",
    "        \n",
    "        if g[beg] != g[end]:\n",
    "            out[beg+1:end] = np.interp(range(beg+1, end), [beg, end], [g[beg], g[end]])\n",
    "        else:\n",
    "            out[beg+1:end] = g[beg]\n",
    "\n",
    "    # Fill in end (if needed)\n",
    "    if end < (len(g) - 1):\n",
    "        beg = value_locs[-1]\n",
    "        fillval = .89 if (g[beg] == 1) else .13\n",
    "        fv.append((g[beg], fillval))\n",
    "\n",
    "        out[beg+1:] = fillval\n",
    "\n",
    "    x['outcome_filled'] = out\n",
    "#    x['outcome'] = x['filled']\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_mean = []\n",
    "leak = []\n",
    "\n",
    "for k in range(len(cv_val)):\n",
    "#for k in [0]:\n",
    "    cv_mean.append(cv_train[k].outcome.mean())\n",
    "    \n",
    "    groups = cv_val[k].group_1.unique()\n",
    "\n",
    "    allGroupsAndDays = pd.DataFrame.from_records(product(groups, alldays))\n",
    "    allGroupsAndDays.columns = ['group_1', 'adate_leak']\n",
    "\n",
    "    meanbycomdate = cv_train[k].groupby(['group_1', 'adate'])['outcome'].agg('mean')\n",
    "\n",
    "    ## Convert the calculation into a proper DataFrame.\n",
    "    meanbycomdate = meanbycomdate.to_frame().reset_index()\n",
    "    meanbycomdate.rename(columns={'adate': 'adate_mean'}, inplace=True)\n",
    "\n",
    "    allGroupsAndDays = pd.merge(allGroupsAndDays, meanbycomdate, left_on=['group_1', 'adate_leak'], right_on=['group_1', 'adate_mean'], how='left')\n",
    "\n",
    "    agad2 = allGroupsAndDays.groupby('group_1').apply(interpolateFun0)\n",
    "    agad2 = agad2.rename(columns={'outcome': 'outcome_leak'})\n",
    "    agad2 = agad2.drop(['adate_mean'], axis=1)\n",
    "    \n",
    "    cv_val_dups[k] = pd.merge(cv_val_dups[k], agad2, left_on=['group_1', 'adate'], right_on=['group_1', 'adate_leak'], how='left')\n",
    "    cv_val_dups[k].drop('adate_leak', axis=1, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4860527911364518, 0.5974661947663344)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = cv_train[0]\n",
    "t.outcome.mean()\n",
    "\n",
    "ta = t[t.outcome_leak2.isnull()]\n",
    "len(ta)\n",
    "t.outcome.mean(), ta.outcome.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(306867, 151727)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_val_dups[3]), len(cv_val[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4860527911364518 0.5974661947663344\n",
      "0.987178087269\n",
      "0.4860559944489479 0.5991144314513376\n",
      "0.987662726824\n",
      "0.4860536013211089 0.5996463392871627\n",
      "0.990469338006\n",
      "0.48605141830116433 0.5971859042156283\n",
      "0.990029341263\n",
      "0.4860541639794943 0.5984634204752134\n",
      "0.987749075407\n",
      "0.48605690968884485 0.5992141008322238\n",
      "0.987918958579\n",
      "0.4860554580581018 0.5988555310595254\n",
      "0.984852696122\n",
      "0.48604996667131134 0.5991608318935842\n",
      "0.969472140622\n",
      "cv: 0.987745455254\n"
     ]
    }
   ],
   "source": [
    "cv_val_dups_tgt_wl = []\n",
    "\n",
    "for k in range(len(cv_val_dups)):\n",
    "        \n",
    "    f = pd.merge(cv_val_dups_tgt[k], cv_val_dups[k][['activity_id', 'outcome_filled', 'outcome_leak', 'outcome_leak2']], on=['activity_id'], how='left')\n",
    "    \n",
    "    tr = cv_train[k]\n",
    "    trl2 = tr[tr.outcome_leak2.isnull()]\n",
    "    \n",
    "    print(tr.outcome.mean(), trl2.outcome.mean())\n",
    "    \n",
    "    #mask = np.where(~f.outcome_leak2.isnull())\n",
    "    #f.outcome_filled.values[mask] = f.outcome_leak2.values[mask]\n",
    "    \n",
    "    f['outcome_filled_nona'] = f.outcome_filled.fillna(tr.outcome.mean())\n",
    "\n",
    "    cv_val_dups_tgt_wl.append(f)\n",
    "    \n",
    "    print(sklearn.metrics.roc_auc_score(f.outcome.values, f.outcome_filled_nona.values))\n",
    "\n",
    "#tmp0 = cv_val_tgt_wl[[0, 2, 3, 4, 5]]\n",
    "tmp = pd.concat(cv_val_dups_tgt_wl)\n",
    "print('cv:', sklearn.metrics.roc_auc_score(tmp.outcome.values, tmp.outcome_filled_nona.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv: 0.987745455254\n"
     ]
    }
   ],
   "source": [
    "train_withleak = pd.merge(train, tmp[['activity_id', 'outcome_filled', 'outcome_filled_nona', 'outcome_leak']], on='activity_id', how='left')\n",
    "\n",
    "print('cv:', sklearn.metrics.roc_auc_score(train_withleak.outcome.values, train_withleak.outcome_filled_nona.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_leak = train_withleak[['activity_id', 'outcome_filled', 'outcome_leak']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2197291"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_leak.to_pickle('train_cvleak8.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# produce test leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = train.group_1.unique()\n",
    "\n",
    "allGroupsAndDays = pd.DataFrame.from_records(product(groups, alldays))\n",
    "allGroupsAndDays.columns = ['group_1', 'adate_leak']\n",
    "\n",
    "meanbycomdate = train.groupby(['group_1', 'adate'])['outcome'].agg('mean')\n",
    "\n",
    "## Convert the calculation into a proper DataFrame.\n",
    "meanbycomdate = meanbycomdate.to_frame().reset_index()\n",
    "meanbycomdate.rename(columns={'adate': 'adate_mean'}, inplace=True)\n",
    "\n",
    "allGroupsAndDays = pd.merge(allGroupsAndDays, meanbycomdate, left_on=['group_1', 'adate_leak'], right_on=['group_1', 'adate_mean'], how='left')\n",
    "\n",
    "agad2 = allGroupsAndDays.groupby('group_1').apply(interpolateFun0)\n",
    "agad2 = agad2.rename(columns={'outcome': 'outcome_leak'})\n",
    "agad2 = agad2.drop(['adate_mean'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_withleak = pd.merge(test, agad2, left_on=['group_1', 'adate'], right_on=['group_1', 'adate_leak'], how='left')\n",
    "test_withleak.drop('adate_leak', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_mean = train.outcome.mean()\n",
    "train_l2 = train[train.outcome_leak2.isnull()]\n",
    "#tr = cv_train[k]\n",
    "#trl2 = tr[tr.outcome_leak2.isnull()]\n",
    "#train_mean  = train_l2.outcome.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mask = np.where(~test.outcome_leak2.isnull())\n",
    "#test_withleak.outcome_filled.values[mask] = test_withleak.outcome_leak2.values[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69073"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_withleak.outcome_filled.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_withleak['outcome_filled_nona'] = test_withleak.outcome_filled.fillna(train_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_out =  test_withleak[['activity_id', 'outcome_filled_nona']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_out.rename(columns={'outcome_filled_nona':'outcome'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_out.to_csv('Submission-cvleak8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train_leak['outcome_filled_nona'] = train_withleak.outcome_filled_nona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Now merge the leak\n",
    "\n",
    "test_leak = test_withleak[['activity_id', 'outcome_filled', 'outcome_filled_nona', 'outcome_leak']]\n",
    "\n",
    "all_leak = pd.concat([train_leak, test_leak])\n",
    "\n",
    "all_leak['outcome_filled_nona'][:len(train_leak)] = train_withleak['outcome_filled_nona']\n",
    "#all_leak.iloc[0:len(train_leak), ['outcome_filled_nona']] = train_withleak['outcome_filled_nona'].values.copy()\n",
    "all_leak['outcome_leak_int'] = all_leak['outcome_leak'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... and copy 0.0/1.0 from outcome_filled to a new entry with likely-good middles filled in\n",
    "mask = all_leak['outcome_filled'] == 1.0\n",
    "mask = np.logical_or(mask, all_leak['outcome_filled'] == 0.0)\n",
    "\n",
    "#np.sum(mask)\n",
    "\n",
    "all_leak.outcome_leak_int.values[np.where(mask)] = all_leak.outcome_filled.values[np.where(mask)]\n",
    "\n",
    "#np.sum(all_leak.outcome_leak.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(all_leak, open('cvleak8-10fold.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
