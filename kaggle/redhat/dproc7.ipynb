{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/cross_validation.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# This file has other stuff, because I thought I'd be starting one-heating in this notebook... but \n",
    "# it turned out to be extra preprocessing instead!\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import datetime\n",
    "from itertools import product\n",
    "from scipy import interpolate ## For other interpolation functions.\n",
    "import time\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "from sklearn.cross_validation import LabelKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with gzip.open('merged7.pkl.gz', 'rb') as fd:\n",
    "    data = pickle.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_category</th>\n",
       "      <th>activity_id</th>\n",
       "      <th>adate</th>\n",
       "      <th>outcome</th>\n",
       "      <th>achar_1</th>\n",
       "      <th>achar_10</th>\n",
       "      <th>achar_2</th>\n",
       "      <th>achar_3</th>\n",
       "      <th>achar_4</th>\n",
       "      <th>achar_5</th>\n",
       "      <th>...</th>\n",
       "      <th>pchar_4</th>\n",
       "      <th>pchar_5</th>\n",
       "      <th>pchar_6</th>\n",
       "      <th>pchar_7</th>\n",
       "      <th>pchar_8</th>\n",
       "      <th>pchar_9</th>\n",
       "      <th>outcome_filled</th>\n",
       "      <th>outcome_filled_nona</th>\n",
       "      <th>outcome_leak</th>\n",
       "      <th>outcome_leak_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1372164</th>\n",
       "      <td>5</td>\n",
       "      <td>act2_3889418</td>\n",
       "      <td>2022-10-20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2251</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         activity_category   activity_id      adate  outcome  achar_1  \\\n",
       "1372164                  5  act2_3889418 2022-10-20      1.0       -1   \n",
       "\n",
       "         achar_10  achar_2  achar_3  achar_4  achar_5        ...         \\\n",
       "1372164      2251       -1       -1       -1       -1        ...          \n",
       "\n",
       "         pchar_4  pchar_5  pchar_6  pchar_7  pchar_8 pchar_9  outcome_filled  \\\n",
       "1372164       24        8        2       10        3       3             1.0   \n",
       "\n",
       "         outcome_filled_nona  outcome_leak  outcome_leak_int  \n",
       "1372164                  1.0           1.0               1.0  \n",
       "\n",
       "[1 rows x 59 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.activity_id == 'act2_3889418']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with gzip.open('cvleak7-10fold.pkl.gz', 'rb') as fd:\n",
    "    cvleak = pickle.load(fd)\n",
    "#cvleak = pickle.load(open('cvleak7-10fold.pkl.gz', 'rb'))\n",
    "\n",
    "data = pd.merge(data, cvleak, on='activity_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['activity_category', 'activity_id', 'adate', 'outcome', 'achar_1',\n",
       "       'achar_10', 'achar_2', 'achar_3', 'achar_4', 'achar_5', 'achar_6',\n",
       "       'achar_7', 'achar_8', 'achar_9', 'people_id', 'pdate', 'pchar_10',\n",
       "       'pchar_11', 'pchar_12', 'pchar_13', 'pchar_14', 'pchar_15', 'pchar_16',\n",
       "       'pchar_17', 'pchar_18', 'pchar_19', 'pchar_20', 'pchar_21', 'pchar_22',\n",
       "       'pchar_23', 'pchar_24', 'pchar_25', 'pchar_26', 'pchar_27', 'pchar_28',\n",
       "       'pchar_29', 'pchar_30', 'pchar_31', 'pchar_32', 'pchar_33', 'pchar_34',\n",
       "       'pchar_35', 'pchar_36', 'pchar_37', 'pchar_38', 'group_1', 'pchar_1',\n",
       "       'pchar_2', 'pchar_3', 'pchar_4', 'pchar_5', 'pchar_6', 'pchar_7',\n",
       "       'pchar_8', 'pchar_9', 'outcome_filled', 'outcome_filled_nona',\n",
       "       'outcome_leak', 'outcome_leak_int'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_traintest():\n",
    "    testset = np.where(data['outcome'].isnull())\n",
    "    trainset = np.where(~data['outcome'].isnull())\n",
    "\n",
    "    return trainset, testset, data.iloc[trainset], data.iloc[testset]\n",
    "\n",
    "trainset, testset, train, test = split_traintest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.outcome_filled_nona.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.984487762204\n",
      "0.0451716364123\n",
      "0.21253620024\n"
     ]
    }
   ],
   "source": [
    "# compute errors of leak backfill (useful references for upcoming NN tests)\n",
    "print(sklearn.metrics.roc_auc_score(train.outcome.values, train.outcome_filled_nona.values)) # AUC\n",
    "\n",
    "print(sklearn.metrics.mean_squared_error(train.outcome.values, train.outcome_filled_nona.values)) # MSE\n",
    "\n",
    "print(np.sqrt(sklearn.metrics.mean_squared_error(train.outcome.values, train.outcome_filled_nona.values))) # RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just for fun, compute AUC of a 10-fold leak interpolation using 4-folds\n",
    "# Looks like the second fold consistently has outliers... so fold 1 should probably be the one worked on first\n",
    "\n",
    "#for i in range(len(cv_val)):\n",
    "#    print(sklearn.metrics.roc_auc_score(cv_val_tgt[i].outcome.values, cv_val[i].outcome_filled.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onehot_values = {}\n",
    "\n",
    "for k in data.keys():\n",
    "    if k == 'pchar_38': # only continuous *char\n",
    "        continue\n",
    "        \n",
    "    if k == 'achar_10': # we're reducing this\n",
    "        continue\n",
    "        \n",
    "    if 'char' in k:\n",
    "        onehot_values[k] = sorted(data[k].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pchar_7 25\n",
      "pchar_6 7\n",
      "achar_4 8\n",
      "pchar_1 2\n",
      "pchar_36 2\n",
      "pchar_8 8\n",
      "pchar_29 2\n",
      "pchar_10 2\n",
      "pchar_32 2\n",
      "achar_3 12\n",
      "pchar_28 2\n",
      "pchar_12 2\n",
      "pchar_25 2\n",
      "pchar_11 2\n",
      "pchar_24 2\n",
      "pchar_14 2\n",
      "pchar_15 2\n",
      "pchar_13 2\n",
      "pchar_17 2\n",
      "pchar_20 2\n",
      "pchar_5 9\n",
      "pchar_30 2\n",
      "pchar_3 43\n",
      "achar_9 20\n",
      "pchar_16 2\n",
      "achar_2 33\n",
      "pchar_33 2\n",
      "pchar_21 2\n",
      "pchar_18 2\n",
      "pchar_4 25\n",
      "achar_5 8\n",
      "pchar_23 2\n",
      "pchar_22 2\n",
      "pchar_34 2\n",
      "achar_1 52\n",
      "pchar_19 2\n",
      "pchar_27 2\n",
      "pchar_35 2\n",
      "pchar_2 3\n",
      "pchar_9 9\n",
      "achar_6 6\n",
      "achar_7 9\n",
      "pchar_31 2\n",
      "achar_8 19\n",
      "pchar_26 2\n",
      "pchar_37 2\n",
      "354\n"
     ]
    }
   ],
   "source": [
    "tot = 0\n",
    "for k in onehot_values.keys():\n",
    "    tot += len(onehot_values[k])\n",
    "    print(k, len(onehot_values[k]))\n",
    "    \n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# achar_10 has 6970 possible values!  figure out which ones are siginificant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(904683, 6516)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train.achar_10.value_counts().values), len(train.achar_10.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "divs = [-1, 1, 4, 16, 40, 100, 200, 500, 1000, 2000, 4000, 8000, 20000, 904684]\n",
    "\n",
    "vc = train.achar_10.value_counts().to_frame().reset_index().rename(columns={'index': 'value','achar_10':'counted'})\n",
    "\n",
    "achar10_lowcounts = {}\n",
    "for i in range(1, len(divs)):\n",
    "    achar10_lowcounts[divs[i]] = vc[np.logical_and(vc['counted'].values > divs[i-1], vc['counted'].values <= divs[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 783 783\n",
      "4 835 1618\n",
      "16 1907 3525\n",
      "40 1115 4640\n",
      "100 818 5458\n",
      "200 466 5924\n",
      "500 347 6271\n",
      "1000 128 6399\n",
      "2000 57 6456\n",
      "4000 32 6488\n",
      "8000 10 6498\n",
      "20000 11 6509\n",
      "904684 7 6516\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for k in sorted(achar10_lowcounts.keys()):\n",
    "    c += len(achar10_lowcounts[k])\n",
    "    print(k, len(achar10_lowcounts[k]), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.443954396573 0.122651583249 0.0609393117382\n"
     ]
    }
   ],
   "source": [
    "tgt_mean = np.mean(train.outcome.values)\n",
    "std_mean = np.std(train.outcome.values)\n",
    "mindiff3 = np.std(train.outcome.values) ** 3.0\n",
    "mindiff4 = np.std(train.outcome.values) ** 4.0\n",
    "print(tgt_mean, mindiff3, mindiff4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build discard list:\n",
    "\n",
    "discardsets = {}\n",
    "\n",
    "discard_sizes = [1, 4, 16, 40]\n",
    "\n",
    "for i in discard_sizes:\n",
    "    discardsets[i] = achar10_lowcounts[i]['value'].values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find test values that do not exist in train set (use on ALL types???)\n",
    "\n",
    "train_values = sorted(train['achar_10'].unique())\n",
    "\n",
    "discard_testonly = []\n",
    "for i in sorted(test['achar_10'].unique()):\n",
    "    if i not in train_values:\n",
    "        discard_testonly.append(i)\n",
    "\n",
    "discardsets[1] = np.concatenate([discardsets[1], np.array(discard_testonly)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_discardset(b, mindiff):\n",
    "    discard = []\n",
    "    for i in achar10_lowcounts[b].iterrows():\n",
    "        ratio = np.sum(train[train.achar_10 == i[1].value].outcome) / i[1].counted\n",
    "        #print(i[1].value, i[1].counted, ratio - tgt_mean)\n",
    "    \n",
    "        if abs(ratio - tgt_mean) < mindiff:\n",
    "            discard.append(i[1]['value'])\n",
    "        \n",
    "    return np.array(discard)\n",
    "\n",
    "discardsets[100] = build_discardset(100, std_mean ** 2)\n",
    "discardsets[200] = build_discardset(200, std_mean ** 2.5)\n",
    "discardsets[400] = build_discardset(500, std_mean ** 3.0)\n",
    "discardsets[1000] = build_discardset(1000, std_mean ** 4.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 23\n",
      "400 160\n",
      "200 245\n",
      "100 499\n",
      "40 1115\n",
      "16 1907\n",
      "4 835\n",
      "1 1237\n"
     ]
    }
   ],
   "source": [
    "achar10_reduced = data.achar_10.values.copy()\n",
    "\n",
    "for k in sorted(discardsets.keys(), reverse=True):\n",
    "    print(k, len(discardsets[k]))\n",
    "    \n",
    "    mask = np.full(len(data), False, dtype=np.bool)\n",
    "    \n",
    "    for i in discardsets[k]:\n",
    "        mask = np.logical_or(mask, data.achar_10 == i)\n",
    "        \n",
    "    achar10_reduced[np.where(mask)] = -k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "956"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(achar10_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1310\n"
     ]
    }
   ],
   "source": [
    "data['achar_10_reduced'] = achar10_reduced\n",
    "onehot_values['achar_10_reduced'] = sorted(data['achar_10_reduced'].unique())\n",
    "\n",
    "tot = 0\n",
    "for k in onehot_values.keys():\n",
    "    tot += len(onehot_values[k])\n",
    "#    print(k, len(onehot_values[k]))\n",
    "    \n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# before (re)building train/test sets, add day of week!\n",
    "\n",
    "data['adate_dayofweek'] = np.array(list(map(lambda x: x.dayofweek, data.adate))) # SLOW!  move out somehow?\n",
    "onehot_values['adate_dayofweek'] = sorted(data['adate_dayofweek'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if True: # Wasteful to recompute a constant every time\n",
    "    mindate = pd.Timestamp('2022-07-17 00:00:00')\n",
    "    maxdate = pd.Timestamp('2023-08-31 00:00:00')\n",
    "    minpdate = pd.Timestamp('2020-05-18 00:00:00')\n",
    "else:\n",
    "    mindate = min(data['date'])\n",
    "    maxdate = max(data['date'])\n",
    "    minpdate = min(data['pdate'])\n",
    "    \n",
    "data['adate_daynum'] = (data.adate - mindate).values.astype('timedelta64[D]')  # fast enough to run anywhere\n",
    "\n",
    "minpdate = pd.Timestamp('2020-05-18 00:00:00')\n",
    "data['pdate_daynum'] = (data.pdate - mindate).values.astype('timedelta64[D]')\n",
    "\n",
    "data['business_days_delta'] = np.busday_count(data.pdate.values.astype('datetime64[D]'), \n",
    "                                              data.adate.values.astype('datetime64[D]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['adate_daynum'] = data.adate_daynum.astype('<m8[D]').values.astype(float)\n",
    "data['pdate_daynum'] = data.pdate_daynum.astype('<m8[D]').values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = data.columns.copy()\n",
    "cols = cols.drop('activity_id')\n",
    "data_dups = data.duplicated(subset=cols)\n",
    "\n",
    "data_dedup = data[~data_dups]\n",
    "#train_cut_dups = train_cut[train_dups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# date gap between events\n",
    "\n",
    "dgap = {}\n",
    "\n",
    "tmp_data = data[['group_1', 'adate_daynum', 'outcome_filled']]\n",
    "\n",
    "#tmp_data_dedup2 = tmp_data_dedup[tmp_data_dedup.duplicated()]\n",
    "\n",
    "groupdays = {}\n",
    "remap = []\n",
    "\n",
    "for g in tmp_data.groupby(['group_1']):\n",
    "    sdays = np.array(sorted(g[1].adate_daynum.unique()))\n",
    "    if len(sdays) == 1:\n",
    "        continue\n",
    "        \n",
    "    sdays_mean = np.array([g[1][g[1].adate_daynum == d].outcome_filled.mean() for d in sdays])\n",
    "    \n",
    "    om_prevday = np.concatenate([[np.nan], sdays_mean[:-1]])\n",
    "    om_nextday = np.concatenate([sdays_mean[1:], [np.nan]])\n",
    "        \n",
    "    diffdays = np.concatenate([[np.nan], np.diff(sdays)])\n",
    "\n",
    "    c = 0\n",
    "    for x in zip(sdays, diffdays, om_prevday, om_nextday):\n",
    "        #remap[(g[0], x[0])] = x[1]\n",
    "        c += 1\n",
    "        remap.append((g[0], x[0], x[1], x[2], x[3]))\n",
    "\n",
    "    '''\n",
    "    if np.sum(sdays_mean == sdays_mean) != 0:\n",
    "        print(sdays)\n",
    "        print(sdays_mean)\n",
    "        print(diffdays)\n",
    "        print(om_prevday)\n",
    "        print(om_nextday)\n",
    "        break\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_remap = pd.DataFrame(remap)\n",
    "\n",
    "df_remap.columns = ['group_1', 'adate_daynum', 'adate_gap', 'outcome_filled_prevday', 'outcome_filled_nextday']\n",
    "\n",
    "subkeys = ['group_1', 'group_1', 'adate_daynum', 'activity_id']\n",
    "\n",
    "data = pd.merge(data, df_remap, on=['group_1', 'adate_daynum'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of people/group\n",
    "\n",
    "ppg = []\n",
    "for g in data.groupby('group_1'):\n",
    "    ppg.append([g[0], len(g[1].people_id.unique())])\n",
    "\n",
    "df_ppg = pd.DataFrame(ppg)\n",
    "df_ppg.columns = ['group_1', 'people_per_group']\n",
    "\n",
    "data = pd.merge(data, df_ppg, on='group_1', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to use dedup'd data here for events/groups+days, since it's per-day and we don't want it distorted\n",
    "\n",
    "cols = data.columns.copy()\n",
    "cols = cols.drop('activity_id')\n",
    "data_dups = data.duplicated(subset=cols)\n",
    "\n",
    "data_dedup = data[~data_dups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppg_d = []\n",
    "for g in data_dedup.groupby(['group_1', 'adate_daynum']):\n",
    "    ppg_d.append([g[0][0], g[0][1], len(g[1].people_id.unique()), len(g[1])])\n",
    "\n",
    "df_ppg_d = pd.DataFrame(ppg_d)\n",
    "\n",
    "df_ppg_d.columns = ['group_1', 'adate_daynum', 'people_per_group_adate', 'events_per_group_adate']\n",
    "\n",
    "data = pd.merge(data, df_ppg_d, on=['group_1', 'adate_daynum'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['activity_category', 'activity_id', 'adate', 'outcome', 'achar_1',\n",
       "       'achar_10', 'achar_2', 'achar_3', 'achar_4', 'achar_5', 'achar_6',\n",
       "       'achar_7', 'achar_8', 'achar_9', 'people_id', 'pdate', 'pchar_10',\n",
       "       'pchar_11', 'pchar_12', 'pchar_13', 'pchar_14', 'pchar_15', 'pchar_16',\n",
       "       'pchar_17', 'pchar_18', 'pchar_19', 'pchar_20', 'pchar_21', 'pchar_22',\n",
       "       'pchar_23', 'pchar_24', 'pchar_25', 'pchar_26', 'pchar_27', 'pchar_28',\n",
       "       'pchar_29', 'pchar_30', 'pchar_31', 'pchar_32', 'pchar_33', 'pchar_34',\n",
       "       'pchar_35', 'pchar_36', 'pchar_37', 'pchar_38', 'group_1', 'pchar_1',\n",
       "       'pchar_2', 'pchar_3', 'pchar_4', 'pchar_5', 'pchar_6', 'pchar_7',\n",
       "       'pchar_8', 'pchar_9', 'outcome_filled', 'outcome_filled_nona',\n",
       "       'outcome_leak', 'outcome_leak_int', 'achar_10_reduced',\n",
       "       'adate_dayofweek', 'adate_daynum', 'pdate_daynum',\n",
       "       'business_days_delta', 'adate_gap', 'outcome_filled_prevday',\n",
       "       'outcome_filled_nextday', 'people_per_group', 'people_per_group_adate',\n",
       "       'events_per_group_adate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.adate_dayofweek = data.adate_dayofweek.astype(np.uint8)\n",
    "\n",
    "data.business_days_delta = data.business_days_delta.astype(np.uint16)\n",
    "\n",
    "data.adate_daynum = data.adate_daynum.astype(np.int16)\n",
    "data.pdate_daynum = data.pdate_daynum.astype(np.int16)\n",
    "\n",
    "data.people_per_group = data.people_per_group.astype(np.int32)\n",
    "data.people_per_group_adate = data.people_per_group_adate.astype(np.int16)\n",
    "data.events_per_group_adate = data.events_per_group_adate.astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_augments = data[['activity_id', 'achar_10_reduced',\n",
    "       'adate_dayofweek', 'adate_daynum', 'pdate_daynum',\n",
    "       'business_days_delta', 'adate_gap', 'outcome_filled_prevday', 'outcome_filled_nextday', 'people_per_group',\n",
    "       'people_per_group_adate', 'events_per_group_adate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activity_id object act1_1 act2_9e+05\n",
      "achar_10_reduced int16 -1000 9151\n",
      "adate_dayofweek uint8 0 6\n",
      "adate_daynum int16 0 410\n",
      "pdate_daynum int16 -790 410\n",
      "business_days_delta uint16 0 856\n",
      "adate_gap float64 1.0 402.0\n",
      "outcome_filled_prevday float64 0.0 1.0\n",
      "outcome_filled_nextday float64 0.0 1.0\n",
      "people_per_group int32 1 77314\n",
      "people_per_group_adate int16 1 2710\n",
      "events_per_group_adate int16 1 4249\n"
     ]
    }
   ],
   "source": [
    "for k in data_augments.keys():\n",
    "    print(k, data_augments[k].dtype, data_augments[k].min(), data_augments[k].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_augments.to_pickle('dproc7.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activity_id                    0\n",
       "achar_10_reduced               0\n",
       "adate_dayofweek                0\n",
       "adate_daynum                   0\n",
       "pdate_daynum                   0\n",
       "business_days_delta            0\n",
       "adate_gap                  81932\n",
       "outcome_filled_prevday    436830\n",
       "outcome_filled_nextday    431203\n",
       "people_per_group               0\n",
       "people_per_group_adate         0\n",
       "events_per_group_adate         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_augments.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_id</th>\n",
       "      <th>achar_10_reduced</th>\n",
       "      <th>adate_dayofweek</th>\n",
       "      <th>adate_daynum</th>\n",
       "      <th>pdate_daynum</th>\n",
       "      <th>business_days_delta</th>\n",
       "      <th>adate_gap</th>\n",
       "      <th>outcome_filled_prevday</th>\n",
       "      <th>outcome_filled_nextday</th>\n",
       "      <th>people_per_group</th>\n",
       "      <th>people_per_group_adate</th>\n",
       "      <th>events_per_group_adate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1372164</th>\n",
       "      <td>act2_3889418</td>\n",
       "      <td>2251</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>74</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          activity_id  achar_10_reduced  adate_dayofweek  adate_daynum  \\\n",
       "1372164  act2_3889418              2251                3            95   \n",
       "\n",
       "         pdate_daynum  business_days_delta  adate_gap  outcome_filled_prevday  \\\n",
       "1372164            74                   15        1.0                     1.0   \n",
       "\n",
       "         outcome_filled_nextday  people_per_group  people_per_group_adate  \\\n",
       "1372164                     1.0                21                       3   \n",
       "\n",
       "         events_per_group_adate  \n",
       "1372164                       4  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_augments[data_augments.activity_id == 'act2_3889418']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
