{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/cross_validation.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import gzip\n",
    "import pickle\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import interpolate ## For other interpolation functions.\n",
    "\n",
    "import sklearn.metrics\n",
    "import sklearn.utils\n",
    "import sklearn.linear_model\n",
    "\n",
    "from sklearn.cross_validation import LabelKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with gzip.open('merged7.pkl.gz', 'rb') as fd:\n",
    "    data = pickle.load(fd)\n",
    "\n",
    "with gzip.open('cvleak7-10fold.pkl.gz', 'rb') as fd:\n",
    "    cvleak = pickle.load(fd)\n",
    "    \n",
    "data = pd.merge(data, cvleak, on='activity_id', how='left')\n",
    "\n",
    "with gzip.open('dproc7.pkl.gz', 'rb') as fd:\n",
    "    extra = pickle.load(fd)\n",
    "\n",
    "data = pd.merge(data, extra, on='activity_id', how='left')\n",
    "\n",
    "if True: # Wasteful to recompute a constant every time\n",
    "    mindate = pd.Timestamp('2022-07-17 00:00:00')\n",
    "    maxdate = pd.Timestamp('2023-08-31 00:00:00')\n",
    "    minpdate = pd.Timestamp('2020-05-18 00:00:00')\n",
    "else:\n",
    "    mindate = min(data['date'])\n",
    "    maxdate = max(data['date'])\n",
    "    minpdate = min(data['pdate'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4439543965728709"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.outcome.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete 17304, it's big and all zeros and may mess up models!\n",
    "\n",
    "data = data[data.group_1 != 17304]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6976975552259174"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.outcome.mean()  # yes, removing that one group skews the outcome THAT much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = data.columns.copy()\n",
    "cols = cols.drop('activity_id')\n",
    "data_dups = data.duplicated(subset=cols)\n",
    "\n",
    "data_dedup = data[~data_dups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dup = data[data_dups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1023194, 1731249)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dedup), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7204231620668224, 0.6655633998573868)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dedup.outcome.mean(), data_dup.outcome.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_traintest():\n",
    "    testset = np.where(data['outcome'].isnull())\n",
    "    trainset = np.where(~data['outcome'].isnull())\n",
    "\n",
    "    return trainset, testset, data.iloc[trainset], data.iloc[testset]\n",
    "\n",
    "trainset, testset, train, test = split_traintest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# skip this on a rerun (or group3+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_group_df(df, classes = True):\n",
    "    procs = {'group_1': [],\n",
    "             'num_people': [],\n",
    "             'num_events': [],\n",
    "             'pdate_mean': [],\n",
    "             'pdate_first': [],\n",
    "             'pdate_latest': [],\n",
    "             'adate_first': [],\n",
    "             'adate_latest': [],\n",
    "             'num_adates': [],\n",
    "             'otype': [],\n",
    "             }\n",
    "\n",
    "    group_class = []\n",
    "\n",
    "    for g in df.groupby('group_1'):\n",
    "        procs['group_1'].append(g[0])\n",
    "\n",
    "        procs['pdate_mean'].append(g[1].pdate_daynum.mean())\n",
    "        procs['pdate_first'].append(g[1].pdate_daynum.min())\n",
    "        procs['pdate_latest'].append(g[1].pdate_daynum.max())\n",
    "\n",
    "        procs['adate_first'].append(g[1].adate_daynum.min())\n",
    "        procs['adate_latest'].append(g[1].adate_daynum.max())\n",
    "\n",
    "        procs['num_people'].append(len(g[1].people_id.unique()))\n",
    "        procs['num_events'].append(len(g[1]))\n",
    "        procs['num_adates'].append(len(g[1].adate_daynum.unique()))\n",
    "\n",
    "        if False:\n",
    "            m = g[1].outcome.mean()\n",
    "            if m == 0:\n",
    "                procs['otype'].append(0)\n",
    "            elif m == 1:\n",
    "                procs['otype'].append(1)\n",
    "            else:\n",
    "                procs['otype'].append(2) # mixed\n",
    "        else:\n",
    "            procs['otype'].append(-1)\n",
    "\n",
    "    df_procs = pd.DataFrame(procs)\n",
    "    return df_procs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_groups = build_group_df(data_dedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_weight_oh(df, key):\n",
    "    weighted = []\n",
    "\n",
    "    numv = df[key].max() + 1\n",
    "    \n",
    "    for g in df.groupby('group_1'):\n",
    "        oh = np.zeros(numv, dtype=np.float64)\n",
    "\n",
    "        vc = g[1][key].value_counts()\n",
    "\n",
    "        wtot = 0\n",
    "        for z in zip(vc.index.values, vc.values):\n",
    "            oh[z[0]] += z[1]\n",
    "\n",
    "        weighted.append(np.hstack([[g[0]], oh / len(g[1])]))\n",
    "\n",
    "    df_wo = pd.DataFrame(weighted)\n",
    "\n",
    "    wo_cols = ['group_1']\n",
    "    for i in range(numv):\n",
    "        wo_cols.append('{0}_group_onehotavg_{1}'.format(key, i))\n",
    "        #print(wo_cols[-1])\n",
    "\n",
    "    df_wo.columns = wo_cols\n",
    "\n",
    "    #data = pd.merge(data, df_w2o, on='group_1', how='left')\n",
    "\n",
    "    droplist = []\n",
    "    for c in df_wo.keys():\n",
    "        #print(c, df_w2o[c].mean())\n",
    "        if df_wo[c].mean() < .001:\n",
    "            droplist.append(c)\n",
    "\n",
    "    for d in droplist:\n",
    "        df_wo.drop(d, axis=1, inplace=True)\n",
    "            \n",
    "    return df_wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oh_keys = []\n",
    "alsouse = ['activity_category']\n",
    "for k in data.keys():\n",
    "    if 'achar_10' in k:\n",
    "        continue\n",
    "        \n",
    "    if 'pchar_38' in k:\n",
    "        continue\n",
    "        \n",
    "    if k in alsouse or 'char' in k:\n",
    "        oh_keys.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activity_category\n",
      "achar_1\n",
      "achar_2\n",
      "achar_3\n",
      "achar_4\n",
      "achar_5\n",
      "achar_6\n",
      "achar_7\n",
      "achar_8\n",
      "achar_9\n",
      "pchar_10\n",
      "pchar_11\n",
      "pchar_12\n",
      "pchar_13\n",
      "pchar_14\n",
      "pchar_15\n",
      "pchar_16\n",
      "pchar_17\n",
      "pchar_18\n",
      "pchar_19\n",
      "pchar_20\n",
      "pchar_21\n",
      "pchar_22\n",
      "pchar_23\n",
      "pchar_24\n",
      "pchar_25\n",
      "pchar_26\n",
      "pchar_27\n",
      "pchar_28\n",
      "pchar_29\n",
      "pchar_30\n",
      "pchar_31\n",
      "pchar_32\n",
      "pchar_33\n",
      "pchar_34\n",
      "pchar_35\n",
      "pchar_36\n",
      "pchar_37\n",
      "pchar_1\n",
      "pchar_2\n",
      "pchar_3\n",
      "pchar_4\n",
      "pchar_5\n",
      "pchar_6\n",
      "pchar_7\n",
      "pchar_8\n",
      "pchar_9\n"
     ]
    }
   ],
   "source": [
    "for k in oh_keys:\n",
    "    print(k)\n",
    "    df_oh = build_weight_oh(data_dedup, k)\n",
    "    df_groups = pd.merge(df_groups, df_oh, on='group_1', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_groups.to_pickle('group2-dfprep1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick up here\n",
    "df_groups = pickle.load(open('group2-dfprep1.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add pchar_38 mean and median\n",
    "\n",
    "procs = {'group_1': [], 'p38_median': [], 'p38_mean': []}\n",
    "\n",
    "for g in data_dedup.groupby('group_1'):\n",
    "    procs['group_1'].append(g[0])\n",
    "\n",
    "    procs['p38_median'].append(g[1].pchar_38.median())\n",
    "    procs['p38_mean'].append(g[1].pchar_38.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_procs = pd.DataFrame(procs)\n",
    "\n",
    "df_groups = pd.merge(df_groups, df_procs, on='group_1', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_groups = train.group_1.unique()\n",
    "test_groups = test.group_1.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_only_groups = []\n",
    "\n",
    "for t in test_groups:\n",
    "    if t not in train_groups:\n",
    "        test_only_groups.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4325"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not pythonic, and slow, but eh\n",
    "mask = np.full(len(df_groups), False, dtype=np.bool)\n",
    "for i in range(len(df_groups)):\n",
    "    if df_groups.iloc[i].group_1 in test_only_groups:\n",
    "        mask[i] = True\n",
    "        \n",
    "np.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_groups.drop('otype', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_groups[~mask].copy()\n",
    "df_test = df_groups[mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "procs = {'group_1': [],\n",
    "         'otype': [],\n",
    "         }\n",
    "\n",
    "group_class = []\n",
    "\n",
    "for g in data_dedup.groupby('group_1'):\n",
    "    procs['group_1'].append(g[0])\n",
    "\n",
    "    m = g[1].outcome.mean()\n",
    "    if m == 0:\n",
    "        procs['otype'].append(0)\n",
    "    elif m == 1:\n",
    "        procs['otype'].append(1)\n",
    "    else:\n",
    "        procs['otype'].append(2) # mixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_procs = pd.DataFrame(procs)\n",
    "\n",
    "df_train = pd.merge(df_train, df_procs, on='group_1', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['adate_first', 'adate_latest', 'group_1', 'num_adates', 'num_events',\n",
       "       'num_people', 'pdate_first', 'pdate_latest', 'pdate_mean',\n",
       "       'activity_category_group_onehotavg_1',\n",
       "       ...\n",
       "       'pchar_9_group_onehotavg_2', 'pchar_9_group_onehotavg_3',\n",
       "       'pchar_9_group_onehotavg_4', 'pchar_9_group_onehotavg_5',\n",
       "       'pchar_9_group_onehotavg_6', 'pchar_9_group_onehotavg_7',\n",
       "       'pchar_9_group_onehotavg_8', 'p38_mean', 'p38_median', 'otype'],\n",
       "      dtype='object', length=291)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adate_first\n",
      "adate_latest\n",
      "group_1\n",
      "num_adates\n",
      "num_events\n",
      "num_people\n",
      "pdate_first\n",
      "pdate_latest\n",
      "pdate_mean\n",
      "activity_category_group_onehotavg_1\n",
      "activity_category_group_onehotavg_2\n",
      "activity_category_group_onehotavg_3\n",
      "activity_category_group_onehotavg_4\n",
      "activity_category_group_onehotavg_5\n",
      "activity_category_group_onehotavg_6\n",
      "activity_category_group_onehotavg_7\n",
      "achar_1_group_onehotavg_0\n",
      "achar_1_group_onehotavg_1\n",
      "achar_1_group_onehotavg_2\n",
      "achar_1_group_onehotavg_3\n",
      "achar_1_group_onehotavg_4\n",
      "achar_1_group_onehotavg_5\n",
      "achar_1_group_onehotavg_6\n",
      "achar_1_group_onehotavg_7\n",
      "achar_1_group_onehotavg_9\n",
      "achar_1_group_onehotavg_10\n",
      "achar_1_group_onehotavg_11\n",
      "achar_1_group_onehotavg_12\n",
      "achar_1_group_onehotavg_14\n",
      "achar_1_group_onehotavg_16\n",
      "achar_1_group_onehotavg_22\n",
      "achar_1_group_onehotavg_24\n",
      "achar_1_group_onehotavg_25\n",
      "achar_1_group_onehotavg_28\n",
      "achar_1_group_onehotavg_51\n",
      "achar_2_group_onehotavg_0\n",
      "achar_2_group_onehotavg_1\n",
      "achar_2_group_onehotavg_2\n",
      "achar_2_group_onehotavg_4\n",
      "achar_2_group_onehotavg_5\n",
      "achar_2_group_onehotavg_6\n",
      "achar_2_group_onehotavg_7\n",
      "achar_2_group_onehotavg_8\n",
      "achar_2_group_onehotavg_9\n",
      "achar_2_group_onehotavg_11\n",
      "achar_2_group_onehotavg_12\n",
      "achar_2_group_onehotavg_13\n",
      "achar_2_group_onehotavg_15\n",
      "achar_2_group_onehotavg_18\n",
      "achar_2_group_onehotavg_30\n",
      "achar_3_group_onehotavg_0\n",
      "achar_3_group_onehotavg_1\n",
      "achar_3_group_onehotavg_2\n",
      "achar_3_group_onehotavg_3\n",
      "achar_3_group_onehotavg_4\n",
      "achar_3_group_onehotavg_5\n",
      "achar_3_group_onehotavg_6\n",
      "achar_3_group_onehotavg_7\n",
      "achar_3_group_onehotavg_8\n",
      "achar_3_group_onehotavg_10\n",
      "achar_4_group_onehotavg_0\n",
      "achar_4_group_onehotavg_1\n",
      "achar_4_group_onehotavg_2\n",
      "achar_4_group_onehotavg_3\n",
      "achar_4_group_onehotavg_4\n",
      "achar_4_group_onehotavg_5\n",
      "achar_4_group_onehotavg_6\n",
      "achar_5_group_onehotavg_0\n",
      "achar_5_group_onehotavg_1\n",
      "achar_5_group_onehotavg_2\n",
      "achar_5_group_onehotavg_3\n",
      "achar_5_group_onehotavg_4\n",
      "achar_5_group_onehotavg_5\n",
      "achar_5_group_onehotavg_6\n",
      "achar_6_group_onehotavg_0\n",
      "achar_6_group_onehotavg_1\n",
      "achar_6_group_onehotavg_2\n",
      "achar_6_group_onehotavg_3\n",
      "achar_6_group_onehotavg_4\n",
      "achar_7_group_onehotavg_0\n",
      "achar_7_group_onehotavg_1\n",
      "achar_7_group_onehotavg_2\n",
      "achar_7_group_onehotavg_3\n",
      "achar_7_group_onehotavg_4\n",
      "achar_7_group_onehotavg_5\n",
      "achar_7_group_onehotavg_6\n",
      "achar_7_group_onehotavg_7\n",
      "achar_8_group_onehotavg_0\n",
      "achar_8_group_onehotavg_1\n",
      "achar_8_group_onehotavg_2\n",
      "achar_8_group_onehotavg_3\n",
      "achar_8_group_onehotavg_4\n",
      "achar_8_group_onehotavg_5\n",
      "achar_8_group_onehotavg_6\n",
      "achar_8_group_onehotavg_7\n",
      "achar_8_group_onehotavg_8\n",
      "achar_8_group_onehotavg_9\n",
      "achar_8_group_onehotavg_10\n",
      "achar_8_group_onehotavg_11\n",
      "achar_8_group_onehotavg_12\n",
      "achar_8_group_onehotavg_13\n",
      "achar_8_group_onehotavg_14\n",
      "achar_8_group_onehotavg_15\n",
      "achar_8_group_onehotavg_17\n",
      "achar_9_group_onehotavg_0\n",
      "achar_9_group_onehotavg_1\n",
      "achar_9_group_onehotavg_2\n",
      "achar_9_group_onehotavg_3\n",
      "achar_9_group_onehotavg_4\n",
      "achar_9_group_onehotavg_5\n",
      "achar_9_group_onehotavg_6\n",
      "achar_9_group_onehotavg_7\n",
      "achar_9_group_onehotavg_9\n",
      "achar_9_group_onehotavg_11\n",
      "achar_9_group_onehotavg_12\n",
      "achar_9_group_onehotavg_13\n",
      "achar_9_group_onehotavg_14\n",
      "achar_9_group_onehotavg_15\n",
      "achar_9_group_onehotavg_16\n",
      "achar_9_group_onehotavg_17\n",
      "achar_9_group_onehotavg_18\n",
      "pchar_10_group_onehotavg_0\n",
      "pchar_10_group_onehotavg_1\n",
      "pchar_11_group_onehotavg_0\n",
      "pchar_11_group_onehotavg_1\n",
      "pchar_12_group_onehotavg_0\n",
      "pchar_12_group_onehotavg_1\n",
      "pchar_13_group_onehotavg_0\n",
      "pchar_13_group_onehotavg_1\n",
      "pchar_14_group_onehotavg_0\n",
      "pchar_14_group_onehotavg_1\n",
      "pchar_15_group_onehotavg_0\n",
      "pchar_15_group_onehotavg_1\n",
      "pchar_16_group_onehotavg_0\n",
      "pchar_16_group_onehotavg_1\n",
      "pchar_17_group_onehotavg_0\n",
      "pchar_17_group_onehotavg_1\n",
      "pchar_18_group_onehotavg_0\n",
      "pchar_18_group_onehotavg_1\n",
      "pchar_19_group_onehotavg_0\n",
      "pchar_19_group_onehotavg_1\n",
      "pchar_20_group_onehotavg_0\n",
      "pchar_20_group_onehotavg_1\n",
      "pchar_21_group_onehotavg_0\n",
      "pchar_21_group_onehotavg_1\n",
      "pchar_22_group_onehotavg_0\n",
      "pchar_22_group_onehotavg_1\n",
      "pchar_23_group_onehotavg_0\n",
      "pchar_23_group_onehotavg_1\n",
      "pchar_24_group_onehotavg_0\n",
      "pchar_24_group_onehotavg_1\n",
      "pchar_25_group_onehotavg_0\n",
      "pchar_25_group_onehotavg_1\n",
      "pchar_26_group_onehotavg_0\n",
      "pchar_26_group_onehotavg_1\n",
      "pchar_27_group_onehotavg_0\n",
      "pchar_27_group_onehotavg_1\n",
      "pchar_28_group_onehotavg_0\n",
      "pchar_28_group_onehotavg_1\n",
      "pchar_29_group_onehotavg_0\n",
      "pchar_29_group_onehotavg_1\n",
      "pchar_30_group_onehotavg_0\n",
      "pchar_30_group_onehotavg_1\n",
      "pchar_31_group_onehotavg_0\n",
      "pchar_31_group_onehotavg_1\n",
      "pchar_32_group_onehotavg_0\n",
      "pchar_32_group_onehotavg_1\n",
      "pchar_33_group_onehotavg_0\n",
      "pchar_33_group_onehotavg_1\n",
      "pchar_34_group_onehotavg_0\n",
      "pchar_34_group_onehotavg_1\n",
      "pchar_35_group_onehotavg_0\n",
      "pchar_35_group_onehotavg_1\n",
      "pchar_36_group_onehotavg_0\n",
      "pchar_36_group_onehotavg_1\n",
      "pchar_37_group_onehotavg_0\n",
      "pchar_37_group_onehotavg_1\n",
      "pchar_1_group_onehotavg_0\n",
      "pchar_1_group_onehotavg_1\n",
      "pchar_2_group_onehotavg_0\n",
      "pchar_2_group_onehotavg_2\n",
      "pchar_3_group_onehotavg_0\n",
      "pchar_3_group_onehotavg_1\n",
      "pchar_3_group_onehotavg_2\n",
      "pchar_3_group_onehotavg_3\n",
      "pchar_3_group_onehotavg_4\n",
      "pchar_3_group_onehotavg_5\n",
      "pchar_3_group_onehotavg_6\n",
      "pchar_3_group_onehotavg_7\n",
      "pchar_3_group_onehotavg_8\n",
      "pchar_3_group_onehotavg_9\n",
      "pchar_3_group_onehotavg_10\n",
      "pchar_3_group_onehotavg_11\n",
      "pchar_3_group_onehotavg_12\n",
      "pchar_3_group_onehotavg_13\n",
      "pchar_3_group_onehotavg_14\n",
      "pchar_3_group_onehotavg_15\n",
      "pchar_3_group_onehotavg_16\n",
      "pchar_3_group_onehotavg_17\n",
      "pchar_3_group_onehotavg_18\n",
      "pchar_3_group_onehotavg_19\n",
      "pchar_3_group_onehotavg_20\n",
      "pchar_3_group_onehotavg_23\n",
      "pchar_3_group_onehotavg_25\n",
      "pchar_3_group_onehotavg_27\n",
      "pchar_3_group_onehotavg_28\n",
      "pchar_3_group_onehotavg_29\n",
      "pchar_3_group_onehotavg_30\n",
      "pchar_3_group_onehotavg_39\n",
      "pchar_4_group_onehotavg_0\n",
      "pchar_4_group_onehotavg_1\n",
      "pchar_4_group_onehotavg_2\n",
      "pchar_4_group_onehotavg_3\n",
      "pchar_4_group_onehotavg_4\n",
      "pchar_4_group_onehotavg_5\n",
      "pchar_4_group_onehotavg_6\n",
      "pchar_4_group_onehotavg_7\n",
      "pchar_4_group_onehotavg_8\n",
      "pchar_4_group_onehotavg_9\n",
      "pchar_4_group_onehotavg_10\n",
      "pchar_4_group_onehotavg_11\n",
      "pchar_4_group_onehotavg_12\n",
      "pchar_4_group_onehotavg_13\n",
      "pchar_4_group_onehotavg_14\n",
      "pchar_4_group_onehotavg_15\n",
      "pchar_4_group_onehotavg_17\n",
      "pchar_4_group_onehotavg_18\n",
      "pchar_4_group_onehotavg_20\n",
      "pchar_4_group_onehotavg_21\n",
      "pchar_4_group_onehotavg_22\n",
      "pchar_4_group_onehotavg_24\n",
      "pchar_5_group_onehotavg_0\n",
      "pchar_5_group_onehotavg_1\n",
      "pchar_5_group_onehotavg_2\n",
      "pchar_5_group_onehotavg_3\n",
      "pchar_5_group_onehotavg_4\n",
      "pchar_5_group_onehotavg_5\n",
      "pchar_5_group_onehotavg_6\n",
      "pchar_5_group_onehotavg_7\n",
      "pchar_5_group_onehotavg_8\n",
      "pchar_6_group_onehotavg_0\n",
      "pchar_6_group_onehotavg_1\n",
      "pchar_6_group_onehotavg_2\n",
      "pchar_6_group_onehotavg_3\n",
      "pchar_6_group_onehotavg_4\n",
      "pchar_6_group_onehotavg_5\n",
      "pchar_7_group_onehotavg_0\n",
      "pchar_7_group_onehotavg_1\n",
      "pchar_7_group_onehotavg_2\n",
      "pchar_7_group_onehotavg_3\n",
      "pchar_7_group_onehotavg_4\n",
      "pchar_7_group_onehotavg_5\n",
      "pchar_7_group_onehotavg_6\n",
      "pchar_7_group_onehotavg_7\n",
      "pchar_7_group_onehotavg_8\n",
      "pchar_7_group_onehotavg_9\n",
      "pchar_7_group_onehotavg_10\n",
      "pchar_7_group_onehotavg_11\n",
      "pchar_7_group_onehotavg_12\n",
      "pchar_7_group_onehotavg_13\n",
      "pchar_7_group_onehotavg_14\n",
      "pchar_7_group_onehotavg_15\n",
      "pchar_7_group_onehotavg_16\n",
      "pchar_7_group_onehotavg_17\n",
      "pchar_7_group_onehotavg_18\n",
      "pchar_7_group_onehotavg_19\n",
      "pchar_7_group_onehotavg_20\n",
      "pchar_7_group_onehotavg_21\n",
      "pchar_7_group_onehotavg_22\n",
      "pchar_7_group_onehotavg_23\n",
      "pchar_7_group_onehotavg_24\n",
      "pchar_8_group_onehotavg_0\n",
      "pchar_8_group_onehotavg_1\n",
      "pchar_8_group_onehotavg_2\n",
      "pchar_8_group_onehotavg_3\n",
      "pchar_8_group_onehotavg_4\n",
      "pchar_8_group_onehotavg_5\n",
      "pchar_8_group_onehotavg_6\n",
      "pchar_8_group_onehotavg_7\n",
      "pchar_9_group_onehotavg_0\n",
      "pchar_9_group_onehotavg_1\n",
      "pchar_9_group_onehotavg_2\n",
      "pchar_9_group_onehotavg_3\n",
      "pchar_9_group_onehotavg_4\n",
      "pchar_9_group_onehotavg_5\n",
      "pchar_9_group_onehotavg_6\n",
      "pchar_9_group_onehotavg_7\n",
      "pchar_9_group_onehotavg_8\n",
      "p38_mean\n",
      "p38_median\n",
      "otype\n"
     ]
    }
   ],
   "source": [
    "for k in df_train.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [    0     1     2 ..., 29892 29894 29895] TEST: [    4     6     7 ..., 29893 29896 29897]\n",
      "TRAIN: [    0     1     2 ..., 29893 29896 29897] TEST: [    8     9    12 ..., 29892 29894 29895]\n",
      "TRAIN: [    4     6     7 ..., 29895 29896 29897] TEST: [    0     1     2 ..., 29880 29884 29886]\n",
      "TRAIN: [    0     1     2 ..., 29895 29896 29897] TEST: [   11    21    23 ..., 29865 29870 29872]\n",
      "TRAIN: [    0     1     2 ..., 29895 29896 29897] TEST: [   10    13    19 ..., 29889 29890 29891]\n"
     ]
    }
   ],
   "source": [
    "# CV prep\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "gtrain = df_train.copy()\n",
    "\n",
    "keys = list(gtrain.keys())\n",
    "keys.remove('otype')\n",
    "#keys.remove('p38_median')\n",
    "#keys.remove('p38_mean')\n",
    "X = gtrain[keys]\n",
    "y = gtrain.otype.values.copy()\n",
    "\n",
    "X_train, X_test = {}, {}\n",
    "y_train, y_test = {}, {}\n",
    "\n",
    "test_indexes = []\n",
    "\n",
    "kf = KFold(len(gtrain), 5, shuffle=True, random_state=0)\n",
    "i = 0\n",
    "for train_index, test_index in kf:\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    np.random.seed(0)\n",
    "    train_i = train_index\n",
    "    test_i = test_index\n",
    "    \n",
    "    test_indexes.append(test_index)\n",
    "    \n",
    "    X_train[i], X_test[i] = X.iloc[train_i], X.iloc[test_i]\n",
    "    y_train[i], y_test[i] = y[train_i], y[test_i]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20 1.02009535567\n",
      "0 30 0.893147499619\n",
      "0 40 0.865105387291\n",
      "0 50 0.857230701889\n",
      "0 60 0.851067679948\n",
      "0 70 0.843657530544\n",
      "0 80 0.841317778139\n",
      "0 90 0.840018177112\n",
      "0 100 0.840300685146\n",
      "0 110 0.839538550077\n",
      "0 120 0.838530078907\n",
      "0 130 0.837719799988\n",
      "0 140 0.832398852174\n",
      "0 150 0.832716607947\n",
      "0 160 0.822760199362\n",
      "0 170 0.823025436424\n",
      "0 180 0.822853190798\n",
      "0 190 0.823031123443\n",
      "0 200 0.822855743471\n",
      "0 210 0.82215488242\n",
      "0 220 0.822117304576\n",
      "0 230 0.821616216866\n",
      "0 240 0.821224979642\n",
      "0 250 0.821107909003\n",
      "0 260 0.821304140235\n",
      "0 270 0.821403873332\n",
      "0 280 0.821567923352\n",
      "0 290 0.821425667321\n",
      "1 20 1.02608482993\n",
      "1 30 0.915669784322\n",
      "1 40 0.866075417748\n",
      "1 50 0.861710090329\n",
      "1 60 0.847826657402\n",
      "1 70 0.841788820994\n",
      "1 80 0.834999083375\n",
      "1 90 0.828881276187\n",
      "1 100 0.827415126791\n",
      "1 110 0.827087256368\n",
      "1 120 0.826102111356\n",
      "1 130 0.825524279755\n",
      "1 140 0.825148429664\n",
      "1 150 0.82482520955\n",
      "1 160 0.819929730884\n",
      "1 170 0.819514245168\n",
      "1 180 0.814894234615\n",
      "1 190 0.814226254969\n",
      "1 200 0.81424253252\n",
      "1 210 0.813879135564\n",
      "1 220 0.813540301931\n",
      "1 230 0.813720124671\n",
      "1 240 0.813858138705\n",
      "1 250 0.813769762372\n",
      "1 260 0.813766803528\n",
      "1 270 0.813610367947\n",
      "1 280 0.81333439724\n",
      "1 290 0.813073914518\n",
      "2 20 1.04693980087\n",
      "2 30 0.941877815682\n",
      "2 40 0.913563685151\n",
      "2 50 0.872840113149\n",
      "2 60 0.864246583914\n",
      "2 70 0.86286984821\n",
      "2 80 0.860109203552\n",
      "2 90 0.854287627704\n",
      "2 100 0.844047519208\n",
      "2 110 0.842197325644\n",
      "2 120 0.836956909712\n",
      "2 130 0.836943993506\n",
      "2 140 0.831556731256\n",
      "2 150 0.830968582963\n",
      "2 160 0.826054511128\n",
      "2 170 0.82613610628\n",
      "2 180 0.826321989977\n",
      "2 190 0.826337676279\n",
      "2 200 0.826234259981\n",
      "2 210 0.825459008069\n",
      "2 220 0.820332721249\n",
      "2 230 0.819839003763\n",
      "2 240 0.819771826936\n",
      "2 250 0.819478097727\n",
      "2 260 0.819249573106\n",
      "2 270 0.818816810499\n",
      "2 280 0.818748029861\n",
      "2 290 0.818848155243\n",
      "3 20 1.09688510749\n",
      "3 30 0.939298193725\n",
      "3 40 0.916228727828\n",
      "3 50 0.891763450827\n",
      "3 60 0.872241533777\n",
      "3 70 0.864595653653\n",
      "3 80 0.85931070981\n",
      "3 90 0.856909899119\n",
      "3 100 0.855009629506\n",
      "3 110 0.849016999573\n",
      "3 120 0.848442355807\n",
      "3 130 0.847647000561\n",
      "3 140 0.848021517997\n",
      "3 150 0.847968156653\n",
      "3 160 0.847897880025\n",
      "3 170 0.847927554177\n",
      "3 180 0.842848127851\n",
      "3 190 0.837940573323\n",
      "3 200 0.837551366575\n",
      "3 210 0.837325365408\n",
      "3 220 0.836651828881\n",
      "3 230 0.836373742408\n",
      "3 240 0.836021049873\n",
      "3 250 0.831105795647\n",
      "3 260 0.831303268573\n",
      "3 270 0.831067544309\n",
      "3 280 0.83116321776\n",
      "3 290 0.831099434511\n",
      "4 20 1.22067209941\n",
      "4 30 1.02551627333\n",
      "4 40 0.962754268803\n",
      "4 50 0.927728212502\n",
      "4 60 0.898612913864\n",
      "4 70 0.891022974336\n",
      "4 80 0.880007939405\n",
      "4 90 0.873442432745\n",
      "4 100 0.863053983142\n",
      "4 110 0.857592338825\n",
      "4 120 0.856728704516\n",
      "4 130 0.850695749901\n",
      "4 140 0.85048147087\n",
      "4 150 0.850927906745\n",
      "4 160 0.851150532097\n",
      "4 170 0.850579244691\n",
      "4 180 0.850096547507\n",
      "4 190 0.849645017101\n",
      "4 200 0.844418339479\n",
      "4 210 0.843794233721\n",
      "4 220 0.838514157657\n",
      "4 230 0.838394135949\n",
      "4 240 0.838503896211\n",
      "4 250 0.838084791488\n",
      "4 260 0.838267708116\n",
      "4 270 0.838286037183\n",
      "4 280 0.838219784163\n",
      "4 290 0.838187621394\n"
     ]
    }
   ],
   "source": [
    "et = {}\n",
    "for f in range(5):\n",
    "    et[f] = ExtraTreesClassifier()\n",
    "    et[f].fit(X_train[f], y_train[f])\n",
    "\n",
    "    for i in range(20, 300, 10):\n",
    "        et[f].set_params(warm_start = True, n_estimators = i)\n",
    "        et[f].fit(X_train[f], y_train[f])\n",
    "        p0 = et[f].predict_proba(X_test[f])\n",
    "        print(f, i, log_loss(y_test[f], p0))\n",
    "\n",
    "df_preds = []\n",
    "for f in range(5):\n",
    "    preds = et[f].predict_proba(X_test[f])\n",
    "    df_preds.append(pd.DataFrame(preds))\n",
    "    df_preds[-1].columns = ['gp_all0', 'gp_all1', 'gp_mixed']\n",
    "    df_preds[-1]['group_1'] = gtrain.iloc[test_indexes[f]].group_1.values\n",
    "\n",
    "df_preds_all = pd.concat(df_preds)\n",
    "\n",
    "train_preds = np.array(df_preds_all.sort_values('group_1').drop('group_1', axis=1))\n",
    "\n",
    "log_loss(gtrain.otype, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20 0.847376410362\n",
      "0 30 0.819595314966\n",
      "0 40 0.816012243128\n",
      "0 50 0.814010303687\n",
      "0 60 0.812475050201\n",
      "0 70 0.810474674305\n",
      "0 80 0.808859015436\n",
      "0 90 0.807923758624\n",
      "0 100 0.807406899485\n",
      "0 110 0.807247172995\n",
      "0 120 0.807087980957\n",
      "0 130 0.806910211261\n",
      "0 140 0.806463733116\n",
      "0 150 0.8063124586\n",
      "0 160 0.806214808484\n",
      "0 170 0.806180594055\n",
      "0 180 0.8061106499\n",
      "0 190 0.806025653759\n",
      "0 200 0.805617413311\n",
      "0 210 0.805675454695\n",
      "0 220 0.805765676756\n",
      "0 230 0.80592751344\n",
      "0 240 0.80565474264\n",
      "0 250 0.805559791837\n",
      "0 260 0.805750638486\n",
      "0 270 0.8056779792\n",
      "0 280 0.805510981426\n",
      "0 290 0.805495972054\n",
      "1 20 0.837397460556\n",
      "1 30 0.824801076006\n",
      "1 40 0.820356492568\n",
      "1 50 0.817919956256\n",
      "1 60 0.815952676013\n",
      "1 70 0.815898028802\n",
      "1 80 0.815113600025\n",
      "1 90 0.815217959331\n",
      "1 100 0.814277242859\n",
      "1 110 0.80901299416\n",
      "1 120 0.808791649101\n",
      "1 130 0.808900795123\n",
      "1 140 0.809031627533\n",
      "1 150 0.808991530429\n",
      "1 160 0.808605375518\n",
      "1 170 0.808234392885\n",
      "1 180 0.808461897322\n",
      "1 190 0.808142424181\n",
      "1 200 0.808240326067\n",
      "1 210 0.807892331445\n",
      "1 220 0.808078045993\n",
      "1 230 0.808212152389\n",
      "1 240 0.808108776474\n",
      "1 250 0.807981141378\n",
      "1 260 0.808025433247\n",
      "1 270 0.808025701625\n",
      "1 280 0.808052885329\n",
      "1 290 0.807849708442\n",
      "2 20 0.82639108903\n",
      "2 30 0.815860010748\n",
      "2 40 0.814874820551\n",
      "2 50 0.811094638735\n",
      "2 60 0.809458666233\n",
      "2 70 0.808528802031\n",
      "2 80 0.807896618055\n",
      "2 90 0.807331085648\n",
      "2 100 0.806224808406\n",
      "2 110 0.805309535254\n",
      "2 120 0.80595323052\n",
      "2 130 0.805484310732\n",
      "2 140 0.800829394332\n",
      "2 150 0.800820713146\n",
      "2 160 0.800368330543\n",
      "2 170 0.800325854495\n",
      "2 180 0.8005920662\n",
      "2 190 0.800221775322\n",
      "2 200 0.800145865149\n",
      "2 210 0.800088888202\n",
      "2 220 0.800064658964\n",
      "2 230 0.799951263569\n",
      "2 240 0.799855779333\n",
      "2 250 0.799721970964\n",
      "2 260 0.799453859497\n",
      "2 270 0.799762783467\n",
      "2 280 0.799705255481\n",
      "2 290 0.799539916901\n",
      "3 20 0.817780608058\n",
      "3 30 0.810160215456\n",
      "3 40 0.807520540911\n",
      "3 50 0.80645443871\n",
      "3 60 0.805629597042\n",
      "3 70 0.805893014847\n",
      "3 80 0.8046690722\n",
      "3 90 0.804264027065\n",
      "3 100 0.804057992554\n",
      "3 110 0.803582109117\n",
      "3 120 0.803447779686\n",
      "3 130 0.803133538964\n",
      "3 140 0.803543676015\n",
      "3 150 0.803201024162\n",
      "3 160 0.803135312422\n",
      "3 170 0.803434412203\n",
      "3 180 0.803115655818\n",
      "3 190 0.802826714122\n",
      "3 200 0.802595500664\n",
      "3 210 0.802551286794\n",
      "3 220 0.802641639429\n",
      "3 230 0.802600148553\n",
      "3 240 0.80259588613\n",
      "3 250 0.802564553296\n",
      "3 260 0.802399435945\n",
      "3 270 0.802546341132\n",
      "3 280 0.802547754506\n",
      "3 290 0.802500186366\n",
      "4 20 0.838199783432\n",
      "4 30 0.825497670035\n",
      "4 40 0.816384126347\n",
      "4 50 0.813718832567\n",
      "4 60 0.811330184459\n",
      "4 70 0.81051880305\n",
      "4 80 0.810344843149\n",
      "4 90 0.810073378256\n",
      "4 100 0.809807098815\n",
      "4 110 0.809367821691\n",
      "4 120 0.809863671321\n",
      "4 130 0.809672390317\n",
      "4 140 0.80900517003\n",
      "4 150 0.808862818809\n",
      "4 160 0.808876613319\n",
      "4 170 0.809060390372\n",
      "4 180 0.809155851844\n",
      "4 190 0.80895006728\n",
      "4 200 0.808988761111\n",
      "4 210 0.809150255902\n",
      "4 220 0.809181870172\n",
      "4 230 0.808940181382\n",
      "4 240 0.809168387324\n",
      "4 250 0.808930937038\n",
      "4 260 0.808695660306\n",
      "4 270 0.808624447982\n",
      "4 280 0.808600036301\n",
      "4 290 0.808420348751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.80476117974112715"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et = {}\n",
    "for f in range(5):\n",
    "    et[f] = ExtraTreesClassifier(min_samples_split=8,min_samples_leaf=2,n_jobs=8)\n",
    "    et[f].fit(X_train[f], y_train[f])\n",
    "\n",
    "    for i in range(20, 300, 10):\n",
    "        et[f].set_params(warm_start = True, n_estimators = i)\n",
    "        et[f].fit(X_train[f], y_train[f])\n",
    "        p0 = et[f].predict_proba(X_test[f])\n",
    "        print(f, i, log_loss(y_test[f], p0))\n",
    "\n",
    "df_preds = []\n",
    "for f in range(5):\n",
    "    preds = et[f].predict_proba(X_test[f])\n",
    "    df_preds.append(pd.DataFrame(preds))\n",
    "    df_preds[-1].columns = ['gp_all0', 'gp_all1', 'gp_mixed']\n",
    "    df_preds[-1]['group_1'] = gtrain.iloc[test_indexes[f]].group_1.values\n",
    "\n",
    "df_preds_all = pd.concat(df_preds)\n",
    "\n",
    "train_preds = np.array(df_preds_all.sort_values('group_1').drop('group_1', axis=1))\n",
    "\n",
    "log_loss(gtrain.otype, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gtesta = df_test.copy()\n",
    "\n",
    "test_preds = []\n",
    "for f in range(5):\n",
    "    test_preds.append(et[f].predict_proba(gtesta))\n",
    "    \n",
    "preds_comb = np.mean([test_preds[j] for j in range(1,5)], axis = 0)\n",
    "\n",
    "df_test_preds = pd.DataFrame(preds_comb)\n",
    "df_test_preds.columns = ['gp_all0', 'gp_all1', 'gp_mixed']\n",
    "df_test_preds['group_1'] = gtesta.group_1.values\n",
    "\n",
    "preds_out = pd.concat([df_test_preds, df_preds_all])\n",
    "\n",
    "preds_out.to_pickle('group3a-preds.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "et_df_test_preds = df_test_preds.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34223, 34223, 29898)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds_out), len(preds_out.group_1.unique()), len(df_preds_all.group_1.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23275862,  0.70258621,  0.06465517],\n",
       "       [ 0.17155172,  0.51982759,  0.30862069],\n",
       "       [ 0.61034483,  0.33017241,  0.05948276],\n",
       "       ..., \n",
       "       [ 0.58706897,  0.4112069 ,  0.00172414],\n",
       "       [ 0.35      ,  0.62155172,  0.02844828],\n",
       "       [ 0.12068966,  0.74137931,  0.13793103]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21710526,  0.72894737,  0.05394737],\n",
       "       [ 0.18421053,  0.49736842,  0.31842105],\n",
       "       [ 0.64078947,  0.31315789,  0.04605263],\n",
       "       ..., \n",
       "       [ 0.52894737,  0.47105263,  0.        ],\n",
       "       [ 0.35921053,  0.61447368,  0.02631579],\n",
       "       [ 0.12631579,  0.74605263,  0.12763158]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23684211,  0.71578947,  0.04736842],\n",
       "       [ 0.17894737,  0.51578947,  0.30526316],\n",
       "       [ 0.55263158,  0.38947368,  0.05789474],\n",
       "       ..., \n",
       "       [ 0.63684211,  0.35789474,  0.00526316],\n",
       "       [ 0.44210526,  0.53157895,  0.02631579],\n",
       "       [ 0.12105263,  0.74736842,  0.13157895]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "et_orig = et.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20 0.817974151135\n",
      "0 30 0.813412032996\n",
      "0 40 0.812392322132\n",
      "0 50 0.811886842968\n",
      "0 60 0.809457785794\n",
      "0 70 0.809058028615\n",
      "0 80 0.808636937309\n",
      "0 90 0.808376464235\n",
      "0 100 0.80831385745\n",
      "0 110 0.807934592503\n",
      "0 120 0.807713919093\n",
      "0 130 0.807456409231\n",
      "0 140 0.807130790297\n",
      "0 150 0.806710266905\n",
      "0 160 0.806860461825\n",
      "0 170 0.806525323377\n",
      "0 180 0.806298659648\n",
      "0 190 0.806008761758\n",
      "0 200 0.806089340607\n",
      "0 210 0.806138551074\n",
      "0 220 0.806123383588\n",
      "0 230 0.806300985585\n",
      "0 240 0.80640922257\n",
      "0 250 0.806314019841\n",
      "0 260 0.806537071777\n",
      "0 270 0.806379445295\n",
      "0 280 0.806248697607\n",
      "0 290 0.806325880333\n",
      "1 20 0.832092037406\n",
      "1 30 0.819917183891\n",
      "1 40 0.815927651964\n",
      "1 50 0.815495186819\n",
      "1 60 0.813603799737\n",
      "1 70 0.812762207605\n",
      "1 80 0.812290602924\n",
      "1 90 0.81202425171\n",
      "1 100 0.811563554483\n",
      "1 110 0.81081866783\n",
      "1 120 0.81038991869\n",
      "1 130 0.810326852703\n",
      "1 140 0.810297011447\n",
      "1 150 0.809359824745\n",
      "1 160 0.809217759314\n",
      "1 170 0.808869542607\n",
      "1 180 0.808689412785\n",
      "1 190 0.808794857292\n",
      "1 200 0.808674698395\n",
      "1 210 0.808600899474\n",
      "1 220 0.808606559405\n",
      "1 230 0.808837816052\n",
      "1 240 0.808567456804\n",
      "1 250 0.808309561745\n",
      "1 260 0.808409607647\n",
      "1 270 0.808597254437\n",
      "1 280 0.808416895574\n",
      "1 290 0.808444274515\n",
      "2 20 0.832733716778\n",
      "2 30 0.822136913853\n",
      "2 40 0.811815614712\n",
      "2 50 0.80487295503\n",
      "2 60 0.803667762729\n",
      "2 70 0.80387863022\n",
      "2 80 0.804109817663\n",
      "2 90 0.803174256187\n",
      "2 100 0.802264211053\n",
      "2 110 0.801921065242\n",
      "2 120 0.801542812326\n",
      "2 130 0.801519640235\n",
      "2 140 0.801085803383\n",
      "2 150 0.800895927605\n",
      "2 160 0.800721535655\n",
      "2 170 0.800321427403\n",
      "2 180 0.800293692597\n",
      "2 190 0.800052625661\n",
      "2 200 0.79987859204\n",
      "2 210 0.799953317358\n",
      "2 220 0.799803379434\n",
      "2 230 0.799721762743\n",
      "2 240 0.799513687318\n",
      "2 250 0.799483459087\n",
      "2 260 0.799649520313\n",
      "2 270 0.799495498011\n",
      "2 280 0.799249960791\n",
      "2 290 0.799310231421\n",
      "3 20 0.814693143818\n",
      "3 30 0.809234401185\n",
      "3 40 0.805760442149\n",
      "3 50 0.805193278719\n",
      "3 60 0.804045490854\n",
      "3 70 0.804439426001\n",
      "3 80 0.80425186694\n",
      "3 90 0.803955646761\n",
      "3 100 0.804330443187\n",
      "3 110 0.804000466836\n",
      "3 120 0.803839440759\n",
      "3 130 0.803790031069\n",
      "3 140 0.80386050275\n",
      "3 150 0.803338072755\n",
      "3 160 0.803487256902\n",
      "3 170 0.803246261424\n",
      "3 180 0.802935988805\n",
      "3 190 0.802757131075\n",
      "3 200 0.802473496118\n",
      "3 210 0.802481422651\n",
      "3 220 0.802445030454\n",
      "3 230 0.802206961516\n",
      "3 240 0.802237841377\n",
      "3 250 0.802353065251\n",
      "3 260 0.802352121716\n",
      "3 270 0.802151082713\n",
      "3 280 0.80217596498\n",
      "3 290 0.802139239789\n",
      "4 20 0.825427316292\n",
      "4 30 0.816365312388\n",
      "4 40 0.814091793583\n",
      "4 50 0.813423259993\n",
      "4 60 0.81328779468\n",
      "4 70 0.812202128316\n",
      "4 80 0.811297943347\n",
      "4 90 0.811552774264\n",
      "4 100 0.810845531048\n",
      "4 110 0.810403992416\n",
      "4 120 0.810503441802\n",
      "4 130 0.810152796109\n",
      "4 140 0.810443081328\n",
      "4 150 0.810084887107\n",
      "4 160 0.809896821793\n",
      "4 170 0.80967470339\n",
      "4 180 0.809681713455\n",
      "4 190 0.80986081843\n",
      "4 200 0.809806408637\n",
      "4 210 0.809824593287\n",
      "4 220 0.809864385202\n",
      "4 230 0.80982649454\n",
      "4 240 0.809368842071\n",
      "4 250 0.809411946987\n",
      "4 260 0.809390350992\n",
      "4 270 0.809409184467\n",
      "4 280 0.808987923029\n",
      "4 290 0.808967432876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8050373772743904"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et = {}\n",
    "for f in range(5):\n",
    "    et[f] = ExtraTreesClassifier(min_samples_split=12,min_samples_leaf=2,n_jobs=8, random_state=12345)\n",
    "    et[f].fit(X_train[f], y_train[f])\n",
    "\n",
    "    for i in range(20, 300, 10):\n",
    "        et[f].set_params(warm_start = True, n_estimators = i)\n",
    "        et[f].fit(X_train[f], y_train[f])\n",
    "        p0 = et[f].predict_proba(X_test[f])\n",
    "        print(f, i, log_loss(y_test[f], p0))\n",
    "\n",
    "df_preds = []\n",
    "for f in range(5):\n",
    "    preds = et[f].predict_proba(X_test[f])\n",
    "    df_preds.append(pd.DataFrame(preds))\n",
    "    df_preds[-1].columns = ['gp_all0', 'gp_all1', 'gp_mixed']\n",
    "    df_preds[-1]['group_1'] = gtrain.iloc[test_indexes[f]].group_1.values\n",
    "\n",
    "df_preds_all = pd.concat(df_preds)\n",
    "\n",
    "train_preds = np.array(df_preds_all.sort_values('group_1').drop('group_1', axis=1))\n",
    "\n",
    "log_loss(gtrain.otype, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xg_train = {}\n",
    "xg_test = {}\n",
    "for f in range(5):\n",
    "    xg_train[f] = xgb.DMatrix( X_train[f], label=y_train[f])\n",
    "    xg_test[f] = xgb.DMatrix(X_test[f], label=y_test[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09498\ttest-mlogloss:1.09512\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 30 rounds.\n",
      "[10]\ttrain-mlogloss:1.06051\ttest-mlogloss:1.06254\n",
      "[20]\ttrain-mlogloss:1.03033\ttest-mlogloss:1.03423\n",
      "[30]\ttrain-mlogloss:1.00362\ttest-mlogloss:1.00929\n",
      "[40]\ttrain-mlogloss:0.979763\ttest-mlogloss:0.98712\n",
      "[50]\ttrain-mlogloss:0.958614\ttest-mlogloss:0.967649\n",
      "[60]\ttrain-mlogloss:0.939315\ttest-mlogloss:0.949892\n",
      "[70]\ttrain-mlogloss:0.922003\ttest-mlogloss:0.934218\n",
      "[80]\ttrain-mlogloss:0.906313\ttest-mlogloss:0.920217\n",
      "[90]\ttrain-mlogloss:0.892022\ttest-mlogloss:0.907591\n",
      "[100]\ttrain-mlogloss:0.879311\ttest-mlogloss:0.896292\n",
      "[110]\ttrain-mlogloss:0.866947\ttest-mlogloss:0.885586\n",
      "[120]\ttrain-mlogloss:0.855923\ttest-mlogloss:0.87614\n",
      "[130]\ttrain-mlogloss:0.845627\ttest-mlogloss:0.867311\n",
      "[140]\ttrain-mlogloss:0.836272\ttest-mlogloss:0.859323\n",
      "[150]\ttrain-mlogloss:0.827842\ttest-mlogloss:0.8523\n",
      "[160]\ttrain-mlogloss:0.819606\ttest-mlogloss:0.845485\n",
      "[170]\ttrain-mlogloss:0.811888\ttest-mlogloss:0.839125\n",
      "[180]\ttrain-mlogloss:0.8047\ttest-mlogloss:0.833418\n",
      "[190]\ttrain-mlogloss:0.798223\ttest-mlogloss:0.828269\n",
      "[200]\ttrain-mlogloss:0.791658\ttest-mlogloss:0.823229\n",
      "[210]\ttrain-mlogloss:0.78565\ttest-mlogloss:0.818657\n",
      "[220]\ttrain-mlogloss:0.779946\ttest-mlogloss:0.814395\n",
      "[230]\ttrain-mlogloss:0.774632\ttest-mlogloss:0.810451\n",
      "[240]\ttrain-mlogloss:0.76957\ttest-mlogloss:0.806726\n",
      "[250]\ttrain-mlogloss:0.764698\ttest-mlogloss:0.803167\n",
      "[260]\ttrain-mlogloss:0.760169\ttest-mlogloss:0.800111\n",
      "[270]\ttrain-mlogloss:0.755672\ttest-mlogloss:0.796973\n",
      "[280]\ttrain-mlogloss:0.751207\ttest-mlogloss:0.794024\n",
      "[290]\ttrain-mlogloss:0.747164\ttest-mlogloss:0.791364\n",
      "[300]\ttrain-mlogloss:0.743197\ttest-mlogloss:0.788853\n",
      "[310]\ttrain-mlogloss:0.73933\ttest-mlogloss:0.786255\n",
      "[320]\ttrain-mlogloss:0.735644\ttest-mlogloss:0.783871\n",
      "[330]\ttrain-mlogloss:0.732142\ttest-mlogloss:0.781796\n",
      "[340]\ttrain-mlogloss:0.728797\ttest-mlogloss:0.779865\n",
      "[350]\ttrain-mlogloss:0.725647\ttest-mlogloss:0.778037\n",
      "[360]\ttrain-mlogloss:0.722685\ttest-mlogloss:0.776349\n",
      "[370]\ttrain-mlogloss:0.719923\ttest-mlogloss:0.774861\n",
      "[380]\ttrain-mlogloss:0.717018\ttest-mlogloss:0.773262\n",
      "[390]\ttrain-mlogloss:0.714198\ttest-mlogloss:0.771728\n",
      "[400]\ttrain-mlogloss:0.711319\ttest-mlogloss:0.770177\n",
      "[410]\ttrain-mlogloss:0.708766\ttest-mlogloss:0.76896\n",
      "[420]\ttrain-mlogloss:0.706432\ttest-mlogloss:0.767845\n",
      "[430]\ttrain-mlogloss:0.703873\ttest-mlogloss:0.766452\n",
      "[440]\ttrain-mlogloss:0.701498\ttest-mlogloss:0.765249\n",
      "[450]\ttrain-mlogloss:0.69929\ttest-mlogloss:0.764247\n",
      "[460]\ttrain-mlogloss:0.696976\ttest-mlogloss:0.763181\n",
      "[470]\ttrain-mlogloss:0.694858\ttest-mlogloss:0.762217\n",
      "[480]\ttrain-mlogloss:0.692877\ttest-mlogloss:0.761439\n",
      "[490]\ttrain-mlogloss:0.690879\ttest-mlogloss:0.760521\n",
      "[500]\ttrain-mlogloss:0.688767\ttest-mlogloss:0.759734\n",
      "[510]\ttrain-mlogloss:0.686698\ttest-mlogloss:0.758942\n",
      "[520]\ttrain-mlogloss:0.684737\ttest-mlogloss:0.758039\n",
      "[530]\ttrain-mlogloss:0.682828\ttest-mlogloss:0.757283\n",
      "[540]\ttrain-mlogloss:0.680976\ttest-mlogloss:0.756533\n",
      "[550]\ttrain-mlogloss:0.679197\ttest-mlogloss:0.755937\n",
      "[560]\ttrain-mlogloss:0.677388\ttest-mlogloss:0.755208\n",
      "[570]\ttrain-mlogloss:0.67551\ttest-mlogloss:0.754627\n",
      "[580]\ttrain-mlogloss:0.673719\ttest-mlogloss:0.753927\n",
      "[590]\ttrain-mlogloss:0.671929\ttest-mlogloss:0.753223\n",
      "[600]\ttrain-mlogloss:0.670157\ttest-mlogloss:0.752573\n",
      "[610]\ttrain-mlogloss:0.668526\ttest-mlogloss:0.751989\n",
      "[620]\ttrain-mlogloss:0.666911\ttest-mlogloss:0.751561\n",
      "[630]\ttrain-mlogloss:0.665261\ttest-mlogloss:0.751046\n",
      "[640]\ttrain-mlogloss:0.663513\ttest-mlogloss:0.750313\n",
      "[650]\ttrain-mlogloss:0.661888\ttest-mlogloss:0.749891\n",
      "[660]\ttrain-mlogloss:0.660452\ttest-mlogloss:0.749487\n",
      "[670]\ttrain-mlogloss:0.659069\ttest-mlogloss:0.749042\n",
      "[680]\ttrain-mlogloss:0.657543\ttest-mlogloss:0.748584\n",
      "[690]\ttrain-mlogloss:0.656067\ttest-mlogloss:0.748226\n",
      "[700]\ttrain-mlogloss:0.654654\ttest-mlogloss:0.747859\n",
      "[710]\ttrain-mlogloss:0.653191\ttest-mlogloss:0.747497\n",
      "[720]\ttrain-mlogloss:0.651806\ttest-mlogloss:0.747087\n",
      "[730]\ttrain-mlogloss:0.650523\ttest-mlogloss:0.746824\n",
      "[740]\ttrain-mlogloss:0.649157\ttest-mlogloss:0.74646\n",
      "[750]\ttrain-mlogloss:0.647717\ttest-mlogloss:0.746105\n",
      "[760]\ttrain-mlogloss:0.646249\ttest-mlogloss:0.745648\n",
      "[770]\ttrain-mlogloss:0.644752\ttest-mlogloss:0.745275\n",
      "[780]\ttrain-mlogloss:0.643374\ttest-mlogloss:0.74486\n",
      "[790]\ttrain-mlogloss:0.641989\ttest-mlogloss:0.744552\n",
      "[800]\ttrain-mlogloss:0.640705\ttest-mlogloss:0.744356\n",
      "[810]\ttrain-mlogloss:0.639236\ttest-mlogloss:0.743906\n",
      "[820]\ttrain-mlogloss:0.638202\ttest-mlogloss:0.743698\n",
      "[830]\ttrain-mlogloss:0.636929\ttest-mlogloss:0.743477\n",
      "[840]\ttrain-mlogloss:0.63566\ttest-mlogloss:0.74324\n",
      "[850]\ttrain-mlogloss:0.634297\ttest-mlogloss:0.743002\n",
      "[860]\ttrain-mlogloss:0.632888\ttest-mlogloss:0.742785\n",
      "[870]\ttrain-mlogloss:0.631648\ttest-mlogloss:0.742548\n",
      "[880]\ttrain-mlogloss:0.630305\ttest-mlogloss:0.742348\n",
      "[890]\ttrain-mlogloss:0.629127\ttest-mlogloss:0.742033\n",
      "[900]\ttrain-mlogloss:0.627874\ttest-mlogloss:0.741897\n",
      "[910]\ttrain-mlogloss:0.626553\ttest-mlogloss:0.74151\n",
      "[920]\ttrain-mlogloss:0.625317\ttest-mlogloss:0.741375\n",
      "[930]\ttrain-mlogloss:0.624014\ttest-mlogloss:0.741145\n",
      "[940]\ttrain-mlogloss:0.622832\ttest-mlogloss:0.740967\n",
      "[950]\ttrain-mlogloss:0.621579\ttest-mlogloss:0.740787\n",
      "[960]\ttrain-mlogloss:0.620452\ttest-mlogloss:0.740558\n",
      "[970]\ttrain-mlogloss:0.619232\ttest-mlogloss:0.740322\n",
      "[980]\ttrain-mlogloss:0.618115\ttest-mlogloss:0.7401\n",
      "[990]\ttrain-mlogloss:0.617015\ttest-mlogloss:0.740019\n",
      "[1000]\ttrain-mlogloss:0.615959\ttest-mlogloss:0.739906\n",
      "[1010]\ttrain-mlogloss:0.614797\ttest-mlogloss:0.739691\n",
      "[1020]\ttrain-mlogloss:0.613606\ttest-mlogloss:0.739584\n",
      "[1030]\ttrain-mlogloss:0.612477\ttest-mlogloss:0.739433\n",
      "[1040]\ttrain-mlogloss:0.611181\ttest-mlogloss:0.739204\n",
      "[1050]\ttrain-mlogloss:0.610024\ttest-mlogloss:0.739141\n",
      "[1060]\ttrain-mlogloss:0.608855\ttest-mlogloss:0.738988\n",
      "[1070]\ttrain-mlogloss:0.607766\ttest-mlogloss:0.738831\n",
      "[1080]\ttrain-mlogloss:0.606724\ttest-mlogloss:0.73872\n",
      "[1090]\ttrain-mlogloss:0.605534\ttest-mlogloss:0.738526\n",
      "[1100]\ttrain-mlogloss:0.604353\ttest-mlogloss:0.738351\n",
      "[1110]\ttrain-mlogloss:0.603292\ttest-mlogloss:0.738281\n",
      "[1120]\ttrain-mlogloss:0.60217\ttest-mlogloss:0.738152\n",
      "[1130]\ttrain-mlogloss:0.601124\ttest-mlogloss:0.73812\n",
      "[1140]\ttrain-mlogloss:0.600199\ttest-mlogloss:0.738069\n",
      "[1150]\ttrain-mlogloss:0.599168\ttest-mlogloss:0.737984\n",
      "[1160]\ttrain-mlogloss:0.598098\ttest-mlogloss:0.737902\n",
      "[1170]\ttrain-mlogloss:0.59706\ttest-mlogloss:0.737844\n",
      "[1180]\ttrain-mlogloss:0.595933\ttest-mlogloss:0.737835\n",
      "[1190]\ttrain-mlogloss:0.594891\ttest-mlogloss:0.737762\n",
      "[1200]\ttrain-mlogloss:0.593839\ttest-mlogloss:0.737689\n",
      "[1210]\ttrain-mlogloss:0.592754\ttest-mlogloss:0.737621\n",
      "[1220]\ttrain-mlogloss:0.591702\ttest-mlogloss:0.737504\n",
      "[1230]\ttrain-mlogloss:0.590607\ttest-mlogloss:0.737365\n",
      "[1240]\ttrain-mlogloss:0.589538\ttest-mlogloss:0.737138\n",
      "[1250]\ttrain-mlogloss:0.5886\ttest-mlogloss:0.737111\n",
      "[1260]\ttrain-mlogloss:0.587536\ttest-mlogloss:0.737001\n",
      "[1270]\ttrain-mlogloss:0.586544\ttest-mlogloss:0.736947\n",
      "[1280]\ttrain-mlogloss:0.585504\ttest-mlogloss:0.736905\n",
      "[1290]\ttrain-mlogloss:0.584506\ttest-mlogloss:0.736829\n",
      "[1300]\ttrain-mlogloss:0.583571\ttest-mlogloss:0.736725\n",
      "[1310]\ttrain-mlogloss:0.582573\ttest-mlogloss:0.736713\n",
      "[1320]\ttrain-mlogloss:0.581573\ttest-mlogloss:0.736703\n",
      "[1330]\ttrain-mlogloss:0.580579\ttest-mlogloss:0.736612\n",
      "[1340]\ttrain-mlogloss:0.579533\ttest-mlogloss:0.736574\n",
      "[1350]\ttrain-mlogloss:0.578528\ttest-mlogloss:0.736502\n",
      "[1360]\ttrain-mlogloss:0.577504\ttest-mlogloss:0.736478\n",
      "[1370]\ttrain-mlogloss:0.576587\ttest-mlogloss:0.736483\n",
      "[1380]\ttrain-mlogloss:0.575624\ttest-mlogloss:0.736431\n",
      "[1390]\ttrain-mlogloss:0.574643\ttest-mlogloss:0.73638\n",
      "[1400]\ttrain-mlogloss:0.573563\ttest-mlogloss:0.736302\n",
      "[1410]\ttrain-mlogloss:0.572546\ttest-mlogloss:0.73622\n",
      "[1420]\ttrain-mlogloss:0.571677\ttest-mlogloss:0.736262\n",
      "[1430]\ttrain-mlogloss:0.570824\ttest-mlogloss:0.736194\n",
      "[1440]\ttrain-mlogloss:0.569814\ttest-mlogloss:0.736111\n",
      "[1450]\ttrain-mlogloss:0.568908\ttest-mlogloss:0.736042\n",
      "[1460]\ttrain-mlogloss:0.568011\ttest-mlogloss:0.736009\n",
      "[1470]\ttrain-mlogloss:0.567111\ttest-mlogloss:0.735952\n",
      "[1480]\ttrain-mlogloss:0.566063\ttest-mlogloss:0.735912\n",
      "[1490]\ttrain-mlogloss:0.565163\ttest-mlogloss:0.735847\n",
      "[1500]\ttrain-mlogloss:0.564319\ttest-mlogloss:0.735831\n",
      "[1510]\ttrain-mlogloss:0.563421\ttest-mlogloss:0.735797\n",
      "[1520]\ttrain-mlogloss:0.562528\ttest-mlogloss:0.735791\n",
      "[1530]\ttrain-mlogloss:0.56159\ttest-mlogloss:0.735756\n",
      "[1540]\ttrain-mlogloss:0.560665\ttest-mlogloss:0.735765\n",
      "[1550]\ttrain-mlogloss:0.559817\ttest-mlogloss:0.735751\n",
      "[1560]\ttrain-mlogloss:0.558878\ttest-mlogloss:0.735735\n",
      "[1570]\ttrain-mlogloss:0.557968\ttest-mlogloss:0.735694\n",
      "[1580]\ttrain-mlogloss:0.557173\ttest-mlogloss:0.735662\n",
      "[1590]\ttrain-mlogloss:0.556277\ttest-mlogloss:0.735687\n",
      "[1600]\ttrain-mlogloss:0.555413\ttest-mlogloss:0.73571\n",
      "[1610]\ttrain-mlogloss:0.554515\ttest-mlogloss:0.735672\n",
      "[1620]\ttrain-mlogloss:0.553699\ttest-mlogloss:0.73566\n",
      "[1630]\ttrain-mlogloss:0.552699\ttest-mlogloss:0.735683\n",
      "[1640]\ttrain-mlogloss:0.551714\ttest-mlogloss:0.735685\n",
      "Stopping. Best iteration:\n",
      "[1612]\ttrain-mlogloss:0.554357\ttest-mlogloss:0.735643\n",
      "\n",
      "[0]\ttrain-mlogloss:1.09491\ttest-mlogloss:1.09514\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 30 rounds.\n",
      "[10]\ttrain-mlogloss:1.06042\ttest-mlogloss:1.06228\n",
      "[20]\ttrain-mlogloss:1.03008\ttest-mlogloss:1.03373\n",
      "[30]\ttrain-mlogloss:1.00317\ttest-mlogloss:1.00822\n",
      "[40]\ttrain-mlogloss:0.979589\ttest-mlogloss:0.986104\n",
      "[50]\ttrain-mlogloss:0.958308\ttest-mlogloss:0.966462\n",
      "[60]\ttrain-mlogloss:0.938992\ttest-mlogloss:0.94859\n",
      "[70]\ttrain-mlogloss:0.922101\ttest-mlogloss:0.933065\n",
      "[80]\ttrain-mlogloss:0.906315\ttest-mlogloss:0.918726\n",
      "[90]\ttrain-mlogloss:0.891835\ttest-mlogloss:0.905894\n",
      "[100]\ttrain-mlogloss:0.879158\ttest-mlogloss:0.894639\n",
      "[110]\ttrain-mlogloss:0.866743\ttest-mlogloss:0.883676\n",
      "[120]\ttrain-mlogloss:0.855626\ttest-mlogloss:0.873854\n",
      "[130]\ttrain-mlogloss:0.845415\ttest-mlogloss:0.865112\n",
      "[140]\ttrain-mlogloss:0.836013\ttest-mlogloss:0.857137\n",
      "[150]\ttrain-mlogloss:0.827504\ttest-mlogloss:0.850107\n",
      "[160]\ttrain-mlogloss:0.819068\ttest-mlogloss:0.842998\n",
      "[170]\ttrain-mlogloss:0.811304\ttest-mlogloss:0.836764\n",
      "[180]\ttrain-mlogloss:0.804324\ttest-mlogloss:0.831342\n",
      "[190]\ttrain-mlogloss:0.79769\ttest-mlogloss:0.826042\n",
      "[200]\ttrain-mlogloss:0.791173\ttest-mlogloss:0.821041\n",
      "[210]\ttrain-mlogloss:0.785501\ttest-mlogloss:0.816745\n",
      "[220]\ttrain-mlogloss:0.780001\ttest-mlogloss:0.8126\n",
      "[230]\ttrain-mlogloss:0.774722\ttest-mlogloss:0.8087\n",
      "[240]\ttrain-mlogloss:0.769635\ttest-mlogloss:0.80501\n",
      "[250]\ttrain-mlogloss:0.764711\ttest-mlogloss:0.801663\n",
      "[260]\ttrain-mlogloss:0.760238\ttest-mlogloss:0.79872\n",
      "[270]\ttrain-mlogloss:0.755688\ttest-mlogloss:0.795698\n",
      "[280]\ttrain-mlogloss:0.751465\ttest-mlogloss:0.792989\n",
      "[290]\ttrain-mlogloss:0.747644\ttest-mlogloss:0.790496\n",
      "[300]\ttrain-mlogloss:0.743637\ttest-mlogloss:0.78801\n",
      "[310]\ttrain-mlogloss:0.73992\ttest-mlogloss:0.785708\n",
      "[320]\ttrain-mlogloss:0.736169\ttest-mlogloss:0.783495\n",
      "[330]\ttrain-mlogloss:0.732742\ttest-mlogloss:0.781468\n",
      "[340]\ttrain-mlogloss:0.729618\ttest-mlogloss:0.779648\n",
      "[350]\ttrain-mlogloss:0.726555\ttest-mlogloss:0.777915\n",
      "[360]\ttrain-mlogloss:0.72361\ttest-mlogloss:0.776284\n",
      "[370]\ttrain-mlogloss:0.72092\ttest-mlogloss:0.774769\n",
      "[380]\ttrain-mlogloss:0.717958\ttest-mlogloss:0.773067\n",
      "[390]\ttrain-mlogloss:0.715158\ttest-mlogloss:0.771516\n",
      "[400]\ttrain-mlogloss:0.712453\ttest-mlogloss:0.770042\n",
      "[410]\ttrain-mlogloss:0.709789\ttest-mlogloss:0.768643\n",
      "[420]\ttrain-mlogloss:0.707163\ttest-mlogloss:0.767352\n",
      "[430]\ttrain-mlogloss:0.704713\ttest-mlogloss:0.766089\n",
      "[440]\ttrain-mlogloss:0.702451\ttest-mlogloss:0.765048\n",
      "[450]\ttrain-mlogloss:0.700234\ttest-mlogloss:0.763982\n",
      "[460]\ttrain-mlogloss:0.697841\ttest-mlogloss:0.762892\n",
      "[470]\ttrain-mlogloss:0.695671\ttest-mlogloss:0.76191\n",
      "[480]\ttrain-mlogloss:0.693353\ttest-mlogloss:0.760898\n",
      "[490]\ttrain-mlogloss:0.691326\ttest-mlogloss:0.760075\n",
      "[500]\ttrain-mlogloss:0.689282\ttest-mlogloss:0.75926\n",
      "[510]\ttrain-mlogloss:0.687313\ttest-mlogloss:0.758548\n",
      "[520]\ttrain-mlogloss:0.68531\ttest-mlogloss:0.757713\n",
      "[530]\ttrain-mlogloss:0.683315\ttest-mlogloss:0.757015\n",
      "[540]\ttrain-mlogloss:0.681559\ttest-mlogloss:0.756366\n",
      "[550]\ttrain-mlogloss:0.679672\ttest-mlogloss:0.755755\n",
      "[560]\ttrain-mlogloss:0.677986\ttest-mlogloss:0.755131\n",
      "[570]\ttrain-mlogloss:0.676184\ttest-mlogloss:0.75453\n",
      "[580]\ttrain-mlogloss:0.674246\ttest-mlogloss:0.753851\n",
      "[590]\ttrain-mlogloss:0.67252\ttest-mlogloss:0.753271\n",
      "[600]\ttrain-mlogloss:0.670806\ttest-mlogloss:0.752614\n",
      "[610]\ttrain-mlogloss:0.669029\ttest-mlogloss:0.751931\n",
      "[620]\ttrain-mlogloss:0.667447\ttest-mlogloss:0.751491\n",
      "[630]\ttrain-mlogloss:0.665849\ttest-mlogloss:0.751015\n",
      "[640]\ttrain-mlogloss:0.664086\ttest-mlogloss:0.750273\n",
      "[650]\ttrain-mlogloss:0.662668\ttest-mlogloss:0.749937\n",
      "[660]\ttrain-mlogloss:0.661176\ttest-mlogloss:0.749557\n",
      "[670]\ttrain-mlogloss:0.659569\ttest-mlogloss:0.749125\n",
      "[680]\ttrain-mlogloss:0.65796\ttest-mlogloss:0.748701\n",
      "[690]\ttrain-mlogloss:0.656518\ttest-mlogloss:0.748317\n",
      "[700]\ttrain-mlogloss:0.654985\ttest-mlogloss:0.747867\n",
      "[710]\ttrain-mlogloss:0.653521\ttest-mlogloss:0.747444\n",
      "[720]\ttrain-mlogloss:0.652282\ttest-mlogloss:0.74718\n",
      "[730]\ttrain-mlogloss:0.650673\ttest-mlogloss:0.746793\n",
      "[740]\ttrain-mlogloss:0.649323\ttest-mlogloss:0.746565\n",
      "[750]\ttrain-mlogloss:0.647901\ttest-mlogloss:0.746259\n",
      "[760]\ttrain-mlogloss:0.646425\ttest-mlogloss:0.745946\n",
      "[770]\ttrain-mlogloss:0.645165\ttest-mlogloss:0.745745\n",
      "[780]\ttrain-mlogloss:0.643698\ttest-mlogloss:0.74548\n",
      "[790]\ttrain-mlogloss:0.642402\ttest-mlogloss:0.745232\n",
      "[800]\ttrain-mlogloss:0.640893\ttest-mlogloss:0.74481\n",
      "[810]\ttrain-mlogloss:0.639573\ttest-mlogloss:0.744586\n",
      "[820]\ttrain-mlogloss:0.638254\ttest-mlogloss:0.744405\n",
      "[830]\ttrain-mlogloss:0.63706\ttest-mlogloss:0.744271\n",
      "[840]\ttrain-mlogloss:0.635674\ttest-mlogloss:0.743919\n",
      "[850]\ttrain-mlogloss:0.634401\ttest-mlogloss:0.74365\n",
      "[860]\ttrain-mlogloss:0.633226\ttest-mlogloss:0.743425\n",
      "[870]\ttrain-mlogloss:0.63203\ttest-mlogloss:0.743251\n",
      "[880]\ttrain-mlogloss:0.630725\ttest-mlogloss:0.743017\n",
      "[890]\ttrain-mlogloss:0.629536\ttest-mlogloss:0.742872\n",
      "[900]\ttrain-mlogloss:0.628366\ttest-mlogloss:0.742679\n",
      "[910]\ttrain-mlogloss:0.627246\ttest-mlogloss:0.742492\n",
      "[920]\ttrain-mlogloss:0.626122\ttest-mlogloss:0.742407\n",
      "[930]\ttrain-mlogloss:0.62496\ttest-mlogloss:0.74228\n",
      "[940]\ttrain-mlogloss:0.623622\ttest-mlogloss:0.742039\n",
      "[950]\ttrain-mlogloss:0.622413\ttest-mlogloss:0.741838\n",
      "[960]\ttrain-mlogloss:0.621156\ttest-mlogloss:0.741601\n",
      "[970]\ttrain-mlogloss:0.620049\ttest-mlogloss:0.741526\n",
      "[980]\ttrain-mlogloss:0.618867\ttest-mlogloss:0.741358\n",
      "[990]\ttrain-mlogloss:0.617744\ttest-mlogloss:0.741253\n",
      "[1000]\ttrain-mlogloss:0.616654\ttest-mlogloss:0.741142\n",
      "[1010]\ttrain-mlogloss:0.615496\ttest-mlogloss:0.74093\n",
      "[1020]\ttrain-mlogloss:0.614266\ttest-mlogloss:0.740805\n",
      "[1030]\ttrain-mlogloss:0.613186\ttest-mlogloss:0.740655\n",
      "[1040]\ttrain-mlogloss:0.611992\ttest-mlogloss:0.740495\n",
      "[1050]\ttrain-mlogloss:0.610849\ttest-mlogloss:0.740387\n",
      "[1060]\ttrain-mlogloss:0.609667\ttest-mlogloss:0.740315\n",
      "[1070]\ttrain-mlogloss:0.608665\ttest-mlogloss:0.740217\n",
      "[1080]\ttrain-mlogloss:0.607595\ttest-mlogloss:0.740114\n",
      "[1090]\ttrain-mlogloss:0.606341\ttest-mlogloss:0.739985\n",
      "[1100]\ttrain-mlogloss:0.60516\ttest-mlogloss:0.739858\n",
      "[1110]\ttrain-mlogloss:0.604159\ttest-mlogloss:0.739803\n",
      "[1120]\ttrain-mlogloss:0.602989\ttest-mlogloss:0.739638\n",
      "[1130]\ttrain-mlogloss:0.601868\ttest-mlogloss:0.739593\n",
      "[1140]\ttrain-mlogloss:0.600761\ttest-mlogloss:0.739553\n",
      "[1150]\ttrain-mlogloss:0.599632\ttest-mlogloss:0.739479\n",
      "[1160]\ttrain-mlogloss:0.59864\ttest-mlogloss:0.73938\n",
      "[1170]\ttrain-mlogloss:0.597445\ttest-mlogloss:0.739228\n",
      "[1180]\ttrain-mlogloss:0.596403\ttest-mlogloss:0.739203\n",
      "[1190]\ttrain-mlogloss:0.595298\ttest-mlogloss:0.739103\n",
      "[1200]\ttrain-mlogloss:0.59428\ttest-mlogloss:0.739002\n",
      "[1210]\ttrain-mlogloss:0.593279\ttest-mlogloss:0.738935\n",
      "[1220]\ttrain-mlogloss:0.592272\ttest-mlogloss:0.738871\n",
      "[1230]\ttrain-mlogloss:0.591262\ttest-mlogloss:0.738788\n",
      "[1240]\ttrain-mlogloss:0.590274\ttest-mlogloss:0.738683\n",
      "[1250]\ttrain-mlogloss:0.589323\ttest-mlogloss:0.738643\n",
      "[1260]\ttrain-mlogloss:0.588382\ttest-mlogloss:0.738612\n",
      "[1270]\ttrain-mlogloss:0.587352\ttest-mlogloss:0.738589\n",
      "[1280]\ttrain-mlogloss:0.586393\ttest-mlogloss:0.738544\n",
      "[1290]\ttrain-mlogloss:0.585425\ttest-mlogloss:0.738479\n",
      "[1300]\ttrain-mlogloss:0.584493\ttest-mlogloss:0.738447\n",
      "[1310]\ttrain-mlogloss:0.58347\ttest-mlogloss:0.738412\n",
      "[1320]\ttrain-mlogloss:0.58255\ttest-mlogloss:0.738388\n",
      "[1330]\ttrain-mlogloss:0.581587\ttest-mlogloss:0.738362\n",
      "[1340]\ttrain-mlogloss:0.58055\ttest-mlogloss:0.738327\n",
      "[1350]\ttrain-mlogloss:0.579608\ttest-mlogloss:0.738289\n",
      "[1360]\ttrain-mlogloss:0.578681\ttest-mlogloss:0.738301\n",
      "[1370]\ttrain-mlogloss:0.577736\ttest-mlogloss:0.738285\n",
      "[1380]\ttrain-mlogloss:0.576853\ttest-mlogloss:0.738282\n",
      "[1390]\ttrain-mlogloss:0.575954\ttest-mlogloss:0.738297\n",
      "Stopping. Best iteration:\n",
      "[1362]\ttrain-mlogloss:0.578515\ttest-mlogloss:0.73827\n",
      "\n",
      "[0]\ttrain-mlogloss:1.09497\ttest-mlogloss:1.09512\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 30 rounds.\n",
      "[10]\ttrain-mlogloss:1.06062\ttest-mlogloss:1.062\n",
      "[20]\ttrain-mlogloss:1.03043\ttest-mlogloss:1.03305\n",
      "[30]\ttrain-mlogloss:1.00377\ttest-mlogloss:1.00758\n",
      "[40]\ttrain-mlogloss:0.980132\ttest-mlogloss:0.985022\n",
      "[50]\ttrain-mlogloss:0.958862\ttest-mlogloss:0.96492\n",
      "[60]\ttrain-mlogloss:0.939658\ttest-mlogloss:0.946921\n",
      "[70]\ttrain-mlogloss:0.922469\ttest-mlogloss:0.930906\n",
      "[80]\ttrain-mlogloss:0.907011\ttest-mlogloss:0.916717\n",
      "[90]\ttrain-mlogloss:0.892756\ttest-mlogloss:0.903859\n",
      "[100]\ttrain-mlogloss:0.880115\ttest-mlogloss:0.892391\n",
      "[110]\ttrain-mlogloss:0.868139\ttest-mlogloss:0.881673\n",
      "[120]\ttrain-mlogloss:0.857023\ttest-mlogloss:0.871968\n",
      "[130]\ttrain-mlogloss:0.846782\ttest-mlogloss:0.86308\n",
      "[140]\ttrain-mlogloss:0.837174\ttest-mlogloss:0.854673\n",
      "[150]\ttrain-mlogloss:0.828752\ttest-mlogloss:0.847526\n",
      "[160]\ttrain-mlogloss:0.820492\ttest-mlogloss:0.840678\n",
      "[170]\ttrain-mlogloss:0.812838\ttest-mlogloss:0.83436\n",
      "[180]\ttrain-mlogloss:0.80564\ttest-mlogloss:0.828651\n",
      "[190]\ttrain-mlogloss:0.798868\ttest-mlogloss:0.823291\n",
      "[200]\ttrain-mlogloss:0.792305\ttest-mlogloss:0.818255\n",
      "[210]\ttrain-mlogloss:0.786474\ttest-mlogloss:0.813705\n",
      "[220]\ttrain-mlogloss:0.780535\ttest-mlogloss:0.809283\n",
      "[230]\ttrain-mlogloss:0.775147\ttest-mlogloss:0.805367\n",
      "[240]\ttrain-mlogloss:0.770219\ttest-mlogloss:0.801773\n",
      "[250]\ttrain-mlogloss:0.765335\ttest-mlogloss:0.798466\n",
      "[260]\ttrain-mlogloss:0.760824\ttest-mlogloss:0.795222\n",
      "[270]\ttrain-mlogloss:0.75618\ttest-mlogloss:0.79204\n",
      "[280]\ttrain-mlogloss:0.751731\ttest-mlogloss:0.789072\n",
      "[290]\ttrain-mlogloss:0.747784\ttest-mlogloss:0.786495\n",
      "[300]\ttrain-mlogloss:0.743809\ttest-mlogloss:0.783889\n",
      "[310]\ttrain-mlogloss:0.740036\ttest-mlogloss:0.781505\n",
      "[320]\ttrain-mlogloss:0.736332\ttest-mlogloss:0.779237\n",
      "[330]\ttrain-mlogloss:0.732655\ttest-mlogloss:0.776946\n",
      "[340]\ttrain-mlogloss:0.729521\ttest-mlogloss:0.77502\n",
      "[350]\ttrain-mlogloss:0.726429\ttest-mlogloss:0.773255\n",
      "[360]\ttrain-mlogloss:0.723372\ttest-mlogloss:0.771493\n",
      "[370]\ttrain-mlogloss:0.720671\ttest-mlogloss:0.76995\n",
      "[380]\ttrain-mlogloss:0.717789\ttest-mlogloss:0.768304\n",
      "[390]\ttrain-mlogloss:0.714808\ttest-mlogloss:0.766612\n",
      "[400]\ttrain-mlogloss:0.712024\ttest-mlogloss:0.765077\n",
      "[410]\ttrain-mlogloss:0.70948\ttest-mlogloss:0.763691\n",
      "[420]\ttrain-mlogloss:0.70702\ttest-mlogloss:0.762545\n",
      "[430]\ttrain-mlogloss:0.704503\ttest-mlogloss:0.76132\n",
      "[440]\ttrain-mlogloss:0.702253\ttest-mlogloss:0.760162\n",
      "[450]\ttrain-mlogloss:0.700087\ttest-mlogloss:0.759248\n",
      "[460]\ttrain-mlogloss:0.697822\ttest-mlogloss:0.758201\n",
      "[470]\ttrain-mlogloss:0.695628\ttest-mlogloss:0.757236\n",
      "[480]\ttrain-mlogloss:0.693383\ttest-mlogloss:0.756119\n",
      "[490]\ttrain-mlogloss:0.691393\ttest-mlogloss:0.755275\n",
      "[500]\ttrain-mlogloss:0.689271\ttest-mlogloss:0.754237\n",
      "[510]\ttrain-mlogloss:0.68729\ttest-mlogloss:0.753441\n",
      "[520]\ttrain-mlogloss:0.685494\ttest-mlogloss:0.752765\n",
      "[530]\ttrain-mlogloss:0.683415\ttest-mlogloss:0.751871\n",
      "[540]\ttrain-mlogloss:0.68171\ttest-mlogloss:0.751252\n",
      "[550]\ttrain-mlogloss:0.679554\ttest-mlogloss:0.750318\n",
      "[560]\ttrain-mlogloss:0.677824\ttest-mlogloss:0.74969\n",
      "[570]\ttrain-mlogloss:0.676107\ttest-mlogloss:0.74907\n",
      "[580]\ttrain-mlogloss:0.67425\ttest-mlogloss:0.748436\n",
      "[590]\ttrain-mlogloss:0.672533\ttest-mlogloss:0.747799\n",
      "[600]\ttrain-mlogloss:0.670903\ttest-mlogloss:0.747163\n",
      "[610]\ttrain-mlogloss:0.669299\ttest-mlogloss:0.746509\n",
      "[620]\ttrain-mlogloss:0.667683\ttest-mlogloss:0.745986\n",
      "[630]\ttrain-mlogloss:0.666022\ttest-mlogloss:0.74548\n",
      "[640]\ttrain-mlogloss:0.664402\ttest-mlogloss:0.744896\n",
      "[650]\ttrain-mlogloss:0.662694\ttest-mlogloss:0.744341\n",
      "[660]\ttrain-mlogloss:0.661032\ttest-mlogloss:0.743698\n",
      "[670]\ttrain-mlogloss:0.659513\ttest-mlogloss:0.743405\n",
      "[680]\ttrain-mlogloss:0.658046\ttest-mlogloss:0.74301\n",
      "[690]\ttrain-mlogloss:0.656406\ttest-mlogloss:0.742528\n",
      "[700]\ttrain-mlogloss:0.655018\ttest-mlogloss:0.742168\n",
      "[710]\ttrain-mlogloss:0.653563\ttest-mlogloss:0.741694\n",
      "[720]\ttrain-mlogloss:0.652156\ttest-mlogloss:0.741328\n",
      "[730]\ttrain-mlogloss:0.650692\ttest-mlogloss:0.740916\n",
      "[740]\ttrain-mlogloss:0.649265\ttest-mlogloss:0.740506\n",
      "[750]\ttrain-mlogloss:0.647823\ttest-mlogloss:0.740049\n",
      "[760]\ttrain-mlogloss:0.646361\ttest-mlogloss:0.739716\n",
      "[770]\ttrain-mlogloss:0.645033\ttest-mlogloss:0.739375\n",
      "[780]\ttrain-mlogloss:0.643602\ttest-mlogloss:0.738928\n",
      "[790]\ttrain-mlogloss:0.642271\ttest-mlogloss:0.738684\n",
      "[800]\ttrain-mlogloss:0.64094\ttest-mlogloss:0.738431\n",
      "[810]\ttrain-mlogloss:0.639588\ttest-mlogloss:0.738099\n",
      "[820]\ttrain-mlogloss:0.638337\ttest-mlogloss:0.737805\n",
      "[830]\ttrain-mlogloss:0.63719\ttest-mlogloss:0.737592\n",
      "[840]\ttrain-mlogloss:0.636068\ttest-mlogloss:0.737424\n",
      "[850]\ttrain-mlogloss:0.63473\ttest-mlogloss:0.737149\n",
      "[860]\ttrain-mlogloss:0.633523\ttest-mlogloss:0.736866\n",
      "[870]\ttrain-mlogloss:0.632346\ttest-mlogloss:0.736634\n",
      "[880]\ttrain-mlogloss:0.63119\ttest-mlogloss:0.736439\n",
      "[890]\ttrain-mlogloss:0.630046\ttest-mlogloss:0.736226\n",
      "[900]\ttrain-mlogloss:0.628871\ttest-mlogloss:0.736065\n",
      "[910]\ttrain-mlogloss:0.627611\ttest-mlogloss:0.735829\n",
      "[920]\ttrain-mlogloss:0.62643\ttest-mlogloss:0.735573\n",
      "[930]\ttrain-mlogloss:0.625408\ttest-mlogloss:0.735374\n",
      "[940]\ttrain-mlogloss:0.624318\ttest-mlogloss:0.735171\n",
      "[950]\ttrain-mlogloss:0.623234\ttest-mlogloss:0.734974\n",
      "[960]\ttrain-mlogloss:0.622186\ttest-mlogloss:0.734794\n",
      "[970]\ttrain-mlogloss:0.621205\ttest-mlogloss:0.734705\n",
      "[980]\ttrain-mlogloss:0.620036\ttest-mlogloss:0.734535\n",
      "[990]\ttrain-mlogloss:0.61899\ttest-mlogloss:0.734402\n",
      "[1000]\ttrain-mlogloss:0.617913\ttest-mlogloss:0.734244\n",
      "[1010]\ttrain-mlogloss:0.617016\ttest-mlogloss:0.734158\n",
      "[1020]\ttrain-mlogloss:0.615815\ttest-mlogloss:0.734023\n",
      "[1030]\ttrain-mlogloss:0.614753\ttest-mlogloss:0.733868\n",
      "[1040]\ttrain-mlogloss:0.613643\ttest-mlogloss:0.733802\n",
      "[1050]\ttrain-mlogloss:0.612591\ttest-mlogloss:0.733664\n",
      "[1060]\ttrain-mlogloss:0.611478\ttest-mlogloss:0.733561\n",
      "[1070]\ttrain-mlogloss:0.610479\ttest-mlogloss:0.733516\n",
      "[1080]\ttrain-mlogloss:0.609416\ttest-mlogloss:0.733356\n",
      "[1090]\ttrain-mlogloss:0.608359\ttest-mlogloss:0.733166\n",
      "[1100]\ttrain-mlogloss:0.607301\ttest-mlogloss:0.733032\n",
      "[1110]\ttrain-mlogloss:0.606283\ttest-mlogloss:0.732941\n",
      "[1120]\ttrain-mlogloss:0.605272\ttest-mlogloss:0.732893\n",
      "[1130]\ttrain-mlogloss:0.604241\ttest-mlogloss:0.732783\n",
      "[1140]\ttrain-mlogloss:0.603236\ttest-mlogloss:0.732709\n",
      "[1150]\ttrain-mlogloss:0.602155\ttest-mlogloss:0.732518\n",
      "[1160]\ttrain-mlogloss:0.601061\ttest-mlogloss:0.732364\n",
      "[1170]\ttrain-mlogloss:0.600054\ttest-mlogloss:0.732308\n",
      "[1180]\ttrain-mlogloss:0.599023\ttest-mlogloss:0.732202\n",
      "[1190]\ttrain-mlogloss:0.597995\ttest-mlogloss:0.732071\n",
      "[1200]\ttrain-mlogloss:0.59711\ttest-mlogloss:0.732011\n",
      "[1210]\ttrain-mlogloss:0.596075\ttest-mlogloss:0.731962\n",
      "[1220]\ttrain-mlogloss:0.595119\ttest-mlogloss:0.7319\n",
      "[1230]\ttrain-mlogloss:0.594227\ttest-mlogloss:0.73178\n",
      "[1240]\ttrain-mlogloss:0.593248\ttest-mlogloss:0.731686\n",
      "[1250]\ttrain-mlogloss:0.592279\ttest-mlogloss:0.7317\n",
      "[1260]\ttrain-mlogloss:0.591339\ttest-mlogloss:0.731692\n",
      "[1270]\ttrain-mlogloss:0.590249\ttest-mlogloss:0.731628\n",
      "[1280]\ttrain-mlogloss:0.589296\ttest-mlogloss:0.731578\n",
      "[1290]\ttrain-mlogloss:0.58813\ttest-mlogloss:0.731474\n",
      "[1300]\ttrain-mlogloss:0.587176\ttest-mlogloss:0.731405\n",
      "[1310]\ttrain-mlogloss:0.586357\ttest-mlogloss:0.731336\n",
      "[1320]\ttrain-mlogloss:0.585423\ttest-mlogloss:0.731354\n",
      "[1330]\ttrain-mlogloss:0.584498\ttest-mlogloss:0.731294\n",
      "[1340]\ttrain-mlogloss:0.583471\ttest-mlogloss:0.731191\n",
      "[1350]\ttrain-mlogloss:0.582634\ttest-mlogloss:0.731162\n",
      "[1360]\ttrain-mlogloss:0.581722\ttest-mlogloss:0.7311\n",
      "[1370]\ttrain-mlogloss:0.580764\ttest-mlogloss:0.731077\n",
      "[1380]\ttrain-mlogloss:0.579781\ttest-mlogloss:0.73105\n",
      "[1390]\ttrain-mlogloss:0.578764\ttest-mlogloss:0.731012\n",
      "[1400]\ttrain-mlogloss:0.577828\ttest-mlogloss:0.730958\n",
      "[1410]\ttrain-mlogloss:0.576924\ttest-mlogloss:0.730982\n",
      "[1420]\ttrain-mlogloss:0.576041\ttest-mlogloss:0.730942\n",
      "[1430]\ttrain-mlogloss:0.575138\ttest-mlogloss:0.730966\n",
      "[1440]\ttrain-mlogloss:0.574257\ttest-mlogloss:0.730941\n",
      "[1450]\ttrain-mlogloss:0.573311\ttest-mlogloss:0.730851\n",
      "[1460]\ttrain-mlogloss:0.572386\ttest-mlogloss:0.730838\n",
      "[1470]\ttrain-mlogloss:0.571392\ttest-mlogloss:0.730752\n",
      "[1480]\ttrain-mlogloss:0.57043\ttest-mlogloss:0.730703\n",
      "[1490]\ttrain-mlogloss:0.569409\ttest-mlogloss:0.730628\n",
      "[1500]\ttrain-mlogloss:0.568541\ttest-mlogloss:0.730625\n",
      "[1510]\ttrain-mlogloss:0.567647\ttest-mlogloss:0.730561\n",
      "[1520]\ttrain-mlogloss:0.566788\ttest-mlogloss:0.73046\n",
      "[1530]\ttrain-mlogloss:0.565873\ttest-mlogloss:0.73043\n",
      "[1540]\ttrain-mlogloss:0.565013\ttest-mlogloss:0.730431\n",
      "[1550]\ttrain-mlogloss:0.564133\ttest-mlogloss:0.73047\n",
      "[1560]\ttrain-mlogloss:0.563312\ttest-mlogloss:0.730402\n",
      "[1570]\ttrain-mlogloss:0.562479\ttest-mlogloss:0.730365\n",
      "[1580]\ttrain-mlogloss:0.561555\ttest-mlogloss:0.730344\n",
      "[1590]\ttrain-mlogloss:0.560648\ttest-mlogloss:0.730275\n",
      "[1600]\ttrain-mlogloss:0.55987\ttest-mlogloss:0.730312\n",
      "[1610]\ttrain-mlogloss:0.558933\ttest-mlogloss:0.730189\n",
      "[1620]\ttrain-mlogloss:0.558105\ttest-mlogloss:0.730174\n",
      "[1630]\ttrain-mlogloss:0.557233\ttest-mlogloss:0.730226\n",
      "[1640]\ttrain-mlogloss:0.556314\ttest-mlogloss:0.730252\n",
      "Stopping. Best iteration:\n",
      "[1618]\ttrain-mlogloss:0.558257\ttest-mlogloss:0.730153\n",
      "\n",
      "[0]\ttrain-mlogloss:1.09501\ttest-mlogloss:1.09506\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 30 rounds.\n",
      "[10]\ttrain-mlogloss:1.06145\ttest-mlogloss:1.06255\n",
      "[20]\ttrain-mlogloss:1.03162\ttest-mlogloss:1.03351\n",
      "[30]\ttrain-mlogloss:1.00546\ttest-mlogloss:1.00838\n",
      "[40]\ttrain-mlogloss:0.981604\ttest-mlogloss:0.985503\n",
      "[50]\ttrain-mlogloss:0.96062\ttest-mlogloss:0.965641\n",
      "[60]\ttrain-mlogloss:0.941454\ttest-mlogloss:0.94743\n",
      "[70]\ttrain-mlogloss:0.92451\ttest-mlogloss:0.931437\n",
      "[80]\ttrain-mlogloss:0.908778\ttest-mlogloss:0.91681\n",
      "[90]\ttrain-mlogloss:0.894749\ttest-mlogloss:0.903912\n",
      "[100]\ttrain-mlogloss:0.88148\ttest-mlogloss:0.89176\n",
      "[110]\ttrain-mlogloss:0.869572\ttest-mlogloss:0.88098\n",
      "[120]\ttrain-mlogloss:0.858422\ttest-mlogloss:0.870935\n",
      "[130]\ttrain-mlogloss:0.848112\ttest-mlogloss:0.861796\n",
      "[140]\ttrain-mlogloss:0.838614\ttest-mlogloss:0.853408\n",
      "[150]\ttrain-mlogloss:0.829715\ttest-mlogloss:0.845598\n",
      "[160]\ttrain-mlogloss:0.821686\ttest-mlogloss:0.838816\n",
      "[170]\ttrain-mlogloss:0.814373\ttest-mlogloss:0.832767\n",
      "[180]\ttrain-mlogloss:0.807214\ttest-mlogloss:0.826754\n",
      "[190]\ttrain-mlogloss:0.800684\ttest-mlogloss:0.821526\n",
      "[200]\ttrain-mlogloss:0.794023\ttest-mlogloss:0.816189\n",
      "[210]\ttrain-mlogloss:0.78784\ttest-mlogloss:0.811318\n",
      "[220]\ttrain-mlogloss:0.782132\ttest-mlogloss:0.806759\n",
      "[230]\ttrain-mlogloss:0.776941\ttest-mlogloss:0.802811\n",
      "[240]\ttrain-mlogloss:0.771537\ttest-mlogloss:0.798875\n",
      "[250]\ttrain-mlogloss:0.766697\ttest-mlogloss:0.795351\n",
      "[260]\ttrain-mlogloss:0.761925\ttest-mlogloss:0.792035\n",
      "[270]\ttrain-mlogloss:0.757664\ttest-mlogloss:0.789109\n",
      "[280]\ttrain-mlogloss:0.75343\ttest-mlogloss:0.78624\n",
      "[290]\ttrain-mlogloss:0.749157\ttest-mlogloss:0.783272\n",
      "[300]\ttrain-mlogloss:0.745239\ttest-mlogloss:0.780806\n",
      "[310]\ttrain-mlogloss:0.741576\ttest-mlogloss:0.77837\n",
      "[320]\ttrain-mlogloss:0.73803\ttest-mlogloss:0.776134\n",
      "[330]\ttrain-mlogloss:0.734567\ttest-mlogloss:0.773948\n",
      "[340]\ttrain-mlogloss:0.730875\ttest-mlogloss:0.771604\n",
      "[350]\ttrain-mlogloss:0.727583\ttest-mlogloss:0.769721\n",
      "[360]\ttrain-mlogloss:0.724625\ttest-mlogloss:0.767947\n",
      "[370]\ttrain-mlogloss:0.721428\ttest-mlogloss:0.766075\n",
      "[380]\ttrain-mlogloss:0.718601\ttest-mlogloss:0.764411\n",
      "[390]\ttrain-mlogloss:0.715937\ttest-mlogloss:0.763032\n",
      "[400]\ttrain-mlogloss:0.713145\ttest-mlogloss:0.761499\n",
      "[410]\ttrain-mlogloss:0.71057\ttest-mlogloss:0.760193\n",
      "[420]\ttrain-mlogloss:0.707965\ttest-mlogloss:0.758989\n",
      "[430]\ttrain-mlogloss:0.7056\ttest-mlogloss:0.757871\n",
      "[440]\ttrain-mlogloss:0.703219\ttest-mlogloss:0.756702\n",
      "[450]\ttrain-mlogloss:0.700892\ttest-mlogloss:0.75561\n",
      "[460]\ttrain-mlogloss:0.698626\ttest-mlogloss:0.754575\n",
      "[470]\ttrain-mlogloss:0.696418\ttest-mlogloss:0.753617\n",
      "[480]\ttrain-mlogloss:0.694298\ttest-mlogloss:0.752668\n",
      "[490]\ttrain-mlogloss:0.692154\ttest-mlogloss:0.751713\n",
      "[500]\ttrain-mlogloss:0.690188\ttest-mlogloss:0.75098\n",
      "[510]\ttrain-mlogloss:0.688064\ttest-mlogloss:0.750171\n",
      "[520]\ttrain-mlogloss:0.686093\ttest-mlogloss:0.749437\n",
      "[530]\ttrain-mlogloss:0.684115\ttest-mlogloss:0.748646\n",
      "[540]\ttrain-mlogloss:0.68226\ttest-mlogloss:0.747906\n",
      "[550]\ttrain-mlogloss:0.680427\ttest-mlogloss:0.747291\n",
      "[560]\ttrain-mlogloss:0.678595\ttest-mlogloss:0.746629\n",
      "[570]\ttrain-mlogloss:0.676738\ttest-mlogloss:0.746028\n",
      "[580]\ttrain-mlogloss:0.674959\ttest-mlogloss:0.745346\n",
      "[590]\ttrain-mlogloss:0.673071\ttest-mlogloss:0.744673\n",
      "[600]\ttrain-mlogloss:0.671241\ttest-mlogloss:0.744015\n",
      "[610]\ttrain-mlogloss:0.669411\ttest-mlogloss:0.74349\n",
      "[620]\ttrain-mlogloss:0.667876\ttest-mlogloss:0.743045\n",
      "[630]\ttrain-mlogloss:0.666261\ttest-mlogloss:0.742513\n",
      "[640]\ttrain-mlogloss:0.664535\ttest-mlogloss:0.741989\n",
      "[650]\ttrain-mlogloss:0.662733\ttest-mlogloss:0.741471\n",
      "[660]\ttrain-mlogloss:0.661115\ttest-mlogloss:0.74101\n",
      "[670]\ttrain-mlogloss:0.659697\ttest-mlogloss:0.740722\n",
      "[680]\ttrain-mlogloss:0.658213\ttest-mlogloss:0.740373\n",
      "[690]\ttrain-mlogloss:0.656729\ttest-mlogloss:0.739884\n",
      "[700]\ttrain-mlogloss:0.655288\ttest-mlogloss:0.739508\n",
      "[710]\ttrain-mlogloss:0.653822\ttest-mlogloss:0.73918\n",
      "[720]\ttrain-mlogloss:0.652335\ttest-mlogloss:0.738863\n",
      "[730]\ttrain-mlogloss:0.650871\ttest-mlogloss:0.738563\n",
      "[740]\ttrain-mlogloss:0.649487\ttest-mlogloss:0.738197\n",
      "[750]\ttrain-mlogloss:0.64799\ttest-mlogloss:0.737876\n",
      "[760]\ttrain-mlogloss:0.646686\ttest-mlogloss:0.737651\n",
      "[770]\ttrain-mlogloss:0.645472\ttest-mlogloss:0.73742\n",
      "[780]\ttrain-mlogloss:0.64418\ttest-mlogloss:0.737119\n",
      "[790]\ttrain-mlogloss:0.642796\ttest-mlogloss:0.736803\n",
      "[800]\ttrain-mlogloss:0.641603\ttest-mlogloss:0.736566\n",
      "[810]\ttrain-mlogloss:0.640284\ttest-mlogloss:0.736323\n",
      "[820]\ttrain-mlogloss:0.638936\ttest-mlogloss:0.736105\n",
      "[830]\ttrain-mlogloss:0.637754\ttest-mlogloss:0.735917\n",
      "[840]\ttrain-mlogloss:0.636489\ttest-mlogloss:0.735739\n",
      "[850]\ttrain-mlogloss:0.63528\ttest-mlogloss:0.735619\n",
      "[860]\ttrain-mlogloss:0.63411\ttest-mlogloss:0.735374\n",
      "[870]\ttrain-mlogloss:0.632944\ttest-mlogloss:0.735189\n",
      "[880]\ttrain-mlogloss:0.63173\ttest-mlogloss:0.735044\n",
      "[890]\ttrain-mlogloss:0.630443\ttest-mlogloss:0.734782\n",
      "[900]\ttrain-mlogloss:0.629245\ttest-mlogloss:0.734647\n",
      "[910]\ttrain-mlogloss:0.628093\ttest-mlogloss:0.734531\n",
      "[920]\ttrain-mlogloss:0.62685\ttest-mlogloss:0.734325\n",
      "[930]\ttrain-mlogloss:0.625632\ttest-mlogloss:0.73405\n",
      "[940]\ttrain-mlogloss:0.624513\ttest-mlogloss:0.733935\n",
      "[950]\ttrain-mlogloss:0.623266\ttest-mlogloss:0.733767\n",
      "[960]\ttrain-mlogloss:0.622142\ttest-mlogloss:0.733628\n",
      "[970]\ttrain-mlogloss:0.62095\ttest-mlogloss:0.733467\n",
      "[980]\ttrain-mlogloss:0.619923\ttest-mlogloss:0.733302\n",
      "[990]\ttrain-mlogloss:0.618725\ttest-mlogloss:0.733145\n",
      "[1000]\ttrain-mlogloss:0.61761\ttest-mlogloss:0.733019\n",
      "[1010]\ttrain-mlogloss:0.616538\ttest-mlogloss:0.732973\n",
      "[1020]\ttrain-mlogloss:0.615432\ttest-mlogloss:0.732836\n",
      "[1030]\ttrain-mlogloss:0.61443\ttest-mlogloss:0.732789\n",
      "[1040]\ttrain-mlogloss:0.61338\ttest-mlogloss:0.732689\n",
      "[1050]\ttrain-mlogloss:0.612268\ttest-mlogloss:0.732567\n",
      "[1060]\ttrain-mlogloss:0.611183\ttest-mlogloss:0.732528\n",
      "[1070]\ttrain-mlogloss:0.610271\ttest-mlogloss:0.732449\n",
      "[1080]\ttrain-mlogloss:0.609265\ttest-mlogloss:0.732367\n",
      "[1090]\ttrain-mlogloss:0.60831\ttest-mlogloss:0.732367\n",
      "[1100]\ttrain-mlogloss:0.607255\ttest-mlogloss:0.732242\n",
      "[1110]\ttrain-mlogloss:0.606291\ttest-mlogloss:0.732218\n",
      "[1120]\ttrain-mlogloss:0.605159\ttest-mlogloss:0.732104\n",
      "[1130]\ttrain-mlogloss:0.604082\ttest-mlogloss:0.732018\n",
      "[1140]\ttrain-mlogloss:0.603052\ttest-mlogloss:0.732005\n",
      "[1150]\ttrain-mlogloss:0.602018\ttest-mlogloss:0.7319\n",
      "[1160]\ttrain-mlogloss:0.601047\ttest-mlogloss:0.731874\n",
      "[1170]\ttrain-mlogloss:0.599935\ttest-mlogloss:0.731793\n",
      "[1180]\ttrain-mlogloss:0.598831\ttest-mlogloss:0.73168\n",
      "[1190]\ttrain-mlogloss:0.597702\ttest-mlogloss:0.731605\n",
      "[1200]\ttrain-mlogloss:0.59667\ttest-mlogloss:0.731547\n",
      "[1210]\ttrain-mlogloss:0.595734\ttest-mlogloss:0.731514\n",
      "[1220]\ttrain-mlogloss:0.594689\ttest-mlogloss:0.731441\n",
      "[1230]\ttrain-mlogloss:0.593702\ttest-mlogloss:0.731356\n",
      "[1240]\ttrain-mlogloss:0.592709\ttest-mlogloss:0.731295\n",
      "[1250]\ttrain-mlogloss:0.591698\ttest-mlogloss:0.731255\n",
      "[1260]\ttrain-mlogloss:0.59062\ttest-mlogloss:0.731161\n",
      "[1270]\ttrain-mlogloss:0.589685\ttest-mlogloss:0.731134\n",
      "[1280]\ttrain-mlogloss:0.588702\ttest-mlogloss:0.731091\n",
      "[1290]\ttrain-mlogloss:0.587722\ttest-mlogloss:0.731076\n",
      "[1300]\ttrain-mlogloss:0.586766\ttest-mlogloss:0.731024\n",
      "[1310]\ttrain-mlogloss:0.585768\ttest-mlogloss:0.730996\n",
      "[1320]\ttrain-mlogloss:0.584734\ttest-mlogloss:0.730976\n",
      "[1330]\ttrain-mlogloss:0.583696\ttest-mlogloss:0.730942\n",
      "[1340]\ttrain-mlogloss:0.582712\ttest-mlogloss:0.730905\n",
      "[1350]\ttrain-mlogloss:0.581804\ttest-mlogloss:0.73089\n",
      "[1360]\ttrain-mlogloss:0.580849\ttest-mlogloss:0.730837\n",
      "[1370]\ttrain-mlogloss:0.579891\ttest-mlogloss:0.730817\n",
      "[1380]\ttrain-mlogloss:0.578923\ttest-mlogloss:0.730827\n",
      "[1390]\ttrain-mlogloss:0.577928\ttest-mlogloss:0.730809\n",
      "[1400]\ttrain-mlogloss:0.577071\ttest-mlogloss:0.730774\n",
      "[1410]\ttrain-mlogloss:0.576119\ttest-mlogloss:0.730719\n",
      "[1420]\ttrain-mlogloss:0.575205\ttest-mlogloss:0.730759\n",
      "[1430]\ttrain-mlogloss:0.574216\ttest-mlogloss:0.730728\n",
      "[1440]\ttrain-mlogloss:0.573405\ttest-mlogloss:0.730781\n",
      "Stopping. Best iteration:\n",
      "[1410]\ttrain-mlogloss:0.576119\ttest-mlogloss:0.730719\n",
      "\n",
      "[0]\ttrain-mlogloss:1.09495\ttest-mlogloss:1.09512\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 30 rounds.\n",
      "[10]\ttrain-mlogloss:1.0608\ttest-mlogloss:1.06245\n",
      "[20]\ttrain-mlogloss:1.03107\ttest-mlogloss:1.03424\n",
      "[30]\ttrain-mlogloss:1.00452\ttest-mlogloss:1.00912\n",
      "[40]\ttrain-mlogloss:0.980247\ttest-mlogloss:0.986127\n",
      "[50]\ttrain-mlogloss:0.95891\ttest-mlogloss:0.966093\n",
      "[60]\ttrain-mlogloss:0.93988\ttest-mlogloss:0.948325\n",
      "[70]\ttrain-mlogloss:0.92283\ttest-mlogloss:0.932596\n",
      "[80]\ttrain-mlogloss:0.907072\ttest-mlogloss:0.918173\n",
      "[90]\ttrain-mlogloss:0.892834\ttest-mlogloss:0.905165\n",
      "[100]\ttrain-mlogloss:0.87963\ttest-mlogloss:0.893302\n",
      "[110]\ttrain-mlogloss:0.867728\ttest-mlogloss:0.882673\n",
      "[120]\ttrain-mlogloss:0.85643\ttest-mlogloss:0.87267\n",
      "[130]\ttrain-mlogloss:0.846073\ttest-mlogloss:0.86356\n",
      "[140]\ttrain-mlogloss:0.836567\ttest-mlogloss:0.85526\n",
      "[150]\ttrain-mlogloss:0.827556\ttest-mlogloss:0.847664\n",
      "[160]\ttrain-mlogloss:0.819492\ttest-mlogloss:0.840757\n",
      "[170]\ttrain-mlogloss:0.811951\ttest-mlogloss:0.834656\n",
      "[180]\ttrain-mlogloss:0.804709\ttest-mlogloss:0.828655\n",
      "[190]\ttrain-mlogloss:0.798356\ttest-mlogloss:0.823555\n",
      "[200]\ttrain-mlogloss:0.791919\ttest-mlogloss:0.818432\n",
      "[210]\ttrain-mlogloss:0.78595\ttest-mlogloss:0.813696\n",
      "[220]\ttrain-mlogloss:0.780156\ttest-mlogloss:0.809262\n",
      "[230]\ttrain-mlogloss:0.774755\ttest-mlogloss:0.805214\n",
      "[240]\ttrain-mlogloss:0.769703\ttest-mlogloss:0.801464\n",
      "[250]\ttrain-mlogloss:0.764841\ttest-mlogloss:0.797931\n",
      "[260]\ttrain-mlogloss:0.760319\ttest-mlogloss:0.794822\n",
      "[270]\ttrain-mlogloss:0.756109\ttest-mlogloss:0.791886\n",
      "[280]\ttrain-mlogloss:0.752018\ttest-mlogloss:0.789121\n",
      "[290]\ttrain-mlogloss:0.748032\ttest-mlogloss:0.786476\n",
      "[300]\ttrain-mlogloss:0.744066\ttest-mlogloss:0.783844\n",
      "[310]\ttrain-mlogloss:0.740361\ttest-mlogloss:0.781386\n",
      "[320]\ttrain-mlogloss:0.736904\ttest-mlogloss:0.779147\n",
      "[330]\ttrain-mlogloss:0.733602\ttest-mlogloss:0.777141\n",
      "[340]\ttrain-mlogloss:0.730005\ttest-mlogloss:0.774782\n",
      "[350]\ttrain-mlogloss:0.7267\ttest-mlogloss:0.772747\n",
      "[360]\ttrain-mlogloss:0.723745\ttest-mlogloss:0.77094\n",
      "[370]\ttrain-mlogloss:0.720838\ttest-mlogloss:0.76909\n",
      "[380]\ttrain-mlogloss:0.718025\ttest-mlogloss:0.767578\n",
      "[390]\ttrain-mlogloss:0.715476\ttest-mlogloss:0.766195\n",
      "[400]\ttrain-mlogloss:0.712706\ttest-mlogloss:0.764774\n",
      "[410]\ttrain-mlogloss:0.710221\ttest-mlogloss:0.763466\n",
      "[420]\ttrain-mlogloss:0.707735\ttest-mlogloss:0.762187\n",
      "[430]\ttrain-mlogloss:0.705385\ttest-mlogloss:0.761061\n",
      "[440]\ttrain-mlogloss:0.702786\ttest-mlogloss:0.759781\n",
      "[450]\ttrain-mlogloss:0.700474\ttest-mlogloss:0.758615\n",
      "[460]\ttrain-mlogloss:0.698309\ttest-mlogloss:0.757671\n",
      "[470]\ttrain-mlogloss:0.696032\ttest-mlogloss:0.756673\n",
      "[480]\ttrain-mlogloss:0.693735\ttest-mlogloss:0.75559\n",
      "[490]\ttrain-mlogloss:0.691732\ttest-mlogloss:0.754742\n",
      "[500]\ttrain-mlogloss:0.68972\ttest-mlogloss:0.753895\n",
      "[510]\ttrain-mlogloss:0.687756\ttest-mlogloss:0.753116\n",
      "[520]\ttrain-mlogloss:0.685793\ttest-mlogloss:0.752324\n",
      "[530]\ttrain-mlogloss:0.683666\ttest-mlogloss:0.751418\n",
      "[540]\ttrain-mlogloss:0.681752\ttest-mlogloss:0.750707\n",
      "[550]\ttrain-mlogloss:0.679885\ttest-mlogloss:0.750042\n",
      "[560]\ttrain-mlogloss:0.678088\ttest-mlogloss:0.749373\n",
      "[570]\ttrain-mlogloss:0.67633\ttest-mlogloss:0.748751\n",
      "[580]\ttrain-mlogloss:0.674605\ttest-mlogloss:0.748154\n",
      "[590]\ttrain-mlogloss:0.672783\ttest-mlogloss:0.747648\n",
      "[600]\ttrain-mlogloss:0.67103\ttest-mlogloss:0.747173\n",
      "[610]\ttrain-mlogloss:0.669335\ttest-mlogloss:0.746602\n",
      "[620]\ttrain-mlogloss:0.66772\ttest-mlogloss:0.746108\n",
      "[630]\ttrain-mlogloss:0.666077\ttest-mlogloss:0.745601\n",
      "[640]\ttrain-mlogloss:0.664609\ttest-mlogloss:0.745105\n",
      "[650]\ttrain-mlogloss:0.663073\ttest-mlogloss:0.744582\n",
      "[660]\ttrain-mlogloss:0.661404\ttest-mlogloss:0.74415\n",
      "[670]\ttrain-mlogloss:0.659968\ttest-mlogloss:0.74376\n",
      "[680]\ttrain-mlogloss:0.658316\ttest-mlogloss:0.743246\n",
      "[690]\ttrain-mlogloss:0.656753\ttest-mlogloss:0.742747\n",
      "[700]\ttrain-mlogloss:0.655118\ttest-mlogloss:0.742267\n",
      "[710]\ttrain-mlogloss:0.653708\ttest-mlogloss:0.741889\n",
      "[720]\ttrain-mlogloss:0.652148\ttest-mlogloss:0.741429\n",
      "[730]\ttrain-mlogloss:0.650741\ttest-mlogloss:0.741118\n",
      "[740]\ttrain-mlogloss:0.649269\ttest-mlogloss:0.740813\n",
      "[750]\ttrain-mlogloss:0.647787\ttest-mlogloss:0.740453\n",
      "[760]\ttrain-mlogloss:0.646408\ttest-mlogloss:0.740119\n",
      "[770]\ttrain-mlogloss:0.645054\ttest-mlogloss:0.739797\n",
      "[780]\ttrain-mlogloss:0.643682\ttest-mlogloss:0.739519\n",
      "[790]\ttrain-mlogloss:0.642413\ttest-mlogloss:0.739308\n",
      "[800]\ttrain-mlogloss:0.641131\ttest-mlogloss:0.739021\n",
      "[810]\ttrain-mlogloss:0.639847\ttest-mlogloss:0.738708\n",
      "[820]\ttrain-mlogloss:0.638521\ttest-mlogloss:0.738481\n",
      "[830]\ttrain-mlogloss:0.637148\ttest-mlogloss:0.738218\n",
      "[840]\ttrain-mlogloss:0.635988\ttest-mlogloss:0.738037\n",
      "[850]\ttrain-mlogloss:0.634796\ttest-mlogloss:0.737809\n",
      "[860]\ttrain-mlogloss:0.633494\ttest-mlogloss:0.737599\n",
      "[870]\ttrain-mlogloss:0.63217\ttest-mlogloss:0.737329\n",
      "[880]\ttrain-mlogloss:0.630823\ttest-mlogloss:0.737071\n",
      "[890]\ttrain-mlogloss:0.629523\ttest-mlogloss:0.736837\n",
      "[900]\ttrain-mlogloss:0.628374\ttest-mlogloss:0.736693\n",
      "[910]\ttrain-mlogloss:0.627326\ttest-mlogloss:0.73653\n",
      "[920]\ttrain-mlogloss:0.626067\ttest-mlogloss:0.736321\n",
      "[930]\ttrain-mlogloss:0.62495\ttest-mlogloss:0.736189\n",
      "[940]\ttrain-mlogloss:0.623761\ttest-mlogloss:0.735987\n",
      "[950]\ttrain-mlogloss:0.622622\ttest-mlogloss:0.735844\n",
      "[960]\ttrain-mlogloss:0.621421\ttest-mlogloss:0.735691\n",
      "[970]\ttrain-mlogloss:0.620318\ttest-mlogloss:0.735564\n",
      "[980]\ttrain-mlogloss:0.619108\ttest-mlogloss:0.735468\n",
      "[990]\ttrain-mlogloss:0.617898\ttest-mlogloss:0.735348\n",
      "[1000]\ttrain-mlogloss:0.616841\ttest-mlogloss:0.735224\n",
      "[1010]\ttrain-mlogloss:0.61575\ttest-mlogloss:0.735107\n",
      "[1020]\ttrain-mlogloss:0.614668\ttest-mlogloss:0.734866\n",
      "[1030]\ttrain-mlogloss:0.613554\ttest-mlogloss:0.734771\n",
      "[1040]\ttrain-mlogloss:0.612518\ttest-mlogloss:0.734704\n",
      "[1050]\ttrain-mlogloss:0.611394\ttest-mlogloss:0.734618\n",
      "[1060]\ttrain-mlogloss:0.610213\ttest-mlogloss:0.7346\n",
      "[1070]\ttrain-mlogloss:0.609107\ttest-mlogloss:0.734455\n",
      "[1080]\ttrain-mlogloss:0.608055\ttest-mlogloss:0.734417\n",
      "[1090]\ttrain-mlogloss:0.606903\ttest-mlogloss:0.734283\n",
      "[1100]\ttrain-mlogloss:0.60583\ttest-mlogloss:0.734077\n",
      "[1110]\ttrain-mlogloss:0.604624\ttest-mlogloss:0.733988\n",
      "[1120]\ttrain-mlogloss:0.603552\ttest-mlogloss:0.733876\n",
      "[1130]\ttrain-mlogloss:0.602538\ttest-mlogloss:0.733798\n",
      "[1140]\ttrain-mlogloss:0.601491\ttest-mlogloss:0.733746\n",
      "[1150]\ttrain-mlogloss:0.600507\ttest-mlogloss:0.733583\n",
      "[1160]\ttrain-mlogloss:0.599304\ttest-mlogloss:0.733457\n",
      "[1170]\ttrain-mlogloss:0.598193\ttest-mlogloss:0.7334\n",
      "[1180]\ttrain-mlogloss:0.59714\ttest-mlogloss:0.733343\n",
      "[1190]\ttrain-mlogloss:0.596174\ttest-mlogloss:0.733289\n",
      "[1200]\ttrain-mlogloss:0.595161\ttest-mlogloss:0.733247\n",
      "[1210]\ttrain-mlogloss:0.594036\ttest-mlogloss:0.733188\n",
      "[1220]\ttrain-mlogloss:0.59303\ttest-mlogloss:0.733118\n",
      "[1230]\ttrain-mlogloss:0.592026\ttest-mlogloss:0.733054\n",
      "[1240]\ttrain-mlogloss:0.591058\ttest-mlogloss:0.733097\n",
      "[1250]\ttrain-mlogloss:0.590138\ttest-mlogloss:0.732992\n",
      "[1260]\ttrain-mlogloss:0.589167\ttest-mlogloss:0.732932\n",
      "[1270]\ttrain-mlogloss:0.588169\ttest-mlogloss:0.732902\n",
      "[1280]\ttrain-mlogloss:0.587192\ttest-mlogloss:0.732819\n",
      "[1290]\ttrain-mlogloss:0.58616\ttest-mlogloss:0.732749\n",
      "[1300]\ttrain-mlogloss:0.585155\ttest-mlogloss:0.732718\n",
      "[1310]\ttrain-mlogloss:0.58422\ttest-mlogloss:0.732706\n",
      "[1320]\ttrain-mlogloss:0.58328\ttest-mlogloss:0.732701\n",
      "[1330]\ttrain-mlogloss:0.582283\ttest-mlogloss:0.732685\n",
      "[1340]\ttrain-mlogloss:0.581239\ttest-mlogloss:0.732617\n",
      "[1350]\ttrain-mlogloss:0.5803\ttest-mlogloss:0.732531\n",
      "[1360]\ttrain-mlogloss:0.579302\ttest-mlogloss:0.732522\n",
      "[1370]\ttrain-mlogloss:0.578354\ttest-mlogloss:0.732481\n",
      "[1380]\ttrain-mlogloss:0.577499\ttest-mlogloss:0.732448\n",
      "[1390]\ttrain-mlogloss:0.576556\ttest-mlogloss:0.732407\n",
      "[1400]\ttrain-mlogloss:0.575666\ttest-mlogloss:0.73246\n",
      "[1410]\ttrain-mlogloss:0.574727\ttest-mlogloss:0.732511\n",
      "[1420]\ttrain-mlogloss:0.573738\ttest-mlogloss:0.732441\n",
      "Stopping. Best iteration:\n",
      "[1390]\ttrain-mlogloss:0.576556\ttest-mlogloss:0.732407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softprob'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.01\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = 3\n",
    "param['subsample'] = 0.7\n",
    "param['colsample_bytree'] = 0.7\n",
    "param['eval_metric'] = 'mlogloss'\n",
    "\n",
    "bst = {}\n",
    "pred = {}\n",
    "for f in range(5):\n",
    "    \n",
    "    watchlist = [ (xg_train[f],'train'), (xg_test[f], 'test') ]\n",
    "    num_round = 2500\n",
    "    bst[f] = xgb.train(param, xg_train[f], num_round, watchlist , verbose_eval = 10, early_stopping_rounds=30)\n",
    "    # get prediction\n",
    "#    pred[f] = bst.predict( xg_test[f] )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73343842413354743"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cv_pred = {}\n",
    "for f in range(5):\n",
    "    xgb_cv_pred[f] = bst[f].predict( xg_test[f], ntree_limit=bst[f].best_ntree_limit)\n",
    "\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "xgb_cv_preds = []\n",
    "df_preds = []\n",
    "for f in range(5):\n",
    "    #preds = et[f].predict_proba(X_test[f])\n",
    "    xgb_cv_pred[f] = bst[f].predict( xg_test[f], ntree_limit=bst[f].best_ntree_limit)\n",
    "    df_preds.append(pd.DataFrame(xgb_cv_pred[f]))\n",
    "    df_preds[-1].columns = ['gp_all0', 'gp_all1', 'gp_mixed']\n",
    "    df_preds[-1]['group_1'] = gtrain.iloc[test_indexes[f]].group_1.values\n",
    "\n",
    "df_preds_all = pd.concat(df_preds)\n",
    "\n",
    "train_preds = np.array(df_preds_all.sort_values('group_1').drop('group_1', axis=1))\n",
    "\n",
    "log_loss(gtrain.otype, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gtesta = df_test.copy()\n",
    "xgb_output = xgb.DMatrix(gtesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "for f in range(5):\n",
    "    test_preds.append(bst[f].predict(xgb_output, ntree_limit = bst[f].best_ntree_limit))\n",
    "    \n",
    "preds_comb = np.mean([test_preds[j] for j in range(1,5)], axis = 0)\n",
    "\n",
    "df_test_preds = pd.DataFrame(preds_comb)\n",
    "df_test_preds.columns = ['gp_all0', 'gp_all1', 'gp_mixed']\n",
    "df_test_preds['group_1'] = gtesta.group_1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_out = pd.concat([df_test_preds, df_preds_all])\n",
    "\n",
    "preds_out.to_pickle('group3d-xgb-preds.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
